<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>microdata-04-microdata-protection-methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="_03-microdata-04-Microdata-protection-methods_files/libs/clipboard/clipboard.min.js"></script>
<script src="_03-microdata-04-Microdata-protection-methods_files/libs/quarto-html/quarto.js"></script>
<script src="_03-microdata-04-Microdata-protection-methods_files/libs/quarto-html/popper.min.js"></script>
<script src="_03-microdata-04-Microdata-protection-methods_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="_03-microdata-04-Microdata-protection-methods_files/libs/quarto-html/anchor.min.js"></script>
<link href="_03-microdata-04-Microdata-protection-methods_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="_03-microdata-04-Microdata-protection-methods_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="_03-microdata-04-Microdata-protection-methods_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="_03-microdata-04-Microdata-protection-methods_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="_03-microdata-04-Microdata-protection-methods_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="microdata-protection-methods" class="level2">
<h2 class="anchored" data-anchor-id="microdata-protection-methods">Microdata protection methods</h2>
<section id="overview-of-concepts-and-methods" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-concepts-and-methods">Overview of concepts and methods</h3>
<p>In this section we explain the basic concepts and methods related to microdata protection. Sections 3.4.2, 3.4.3, 3.4.4 and 3.4.5 give in-depth descriptions of some particularly complex methods: microaggregation, rank swapping, additive noise and synthetic data (the first two implemented in μ‑ARGUS).</p>
<p>A microdata set <span class="math inline">\(V\)</span> can be viewed as a file with <span class="math inline">\(n\)</span> records, where each record contains <span class="math inline">\(m\)</span> variables on an individual respondent. The variables can be classified in four categories which are not necessarily disjoint:</p>
<ul>
<li><em>Identifiers</em>. These are variables that <em>unambiguously</em> identify the respondent. Examples are the passport number, social security number, etc.</li>
<li><em>Quasi-identifiers or key variables</em>. These are variables which identify the respondent with some degree of ambiguity. (Nonetheless, a combination of quasi-identifiers may provide unambiguous identification.) Examples are name, address, gender, age, telephone number, etc.</li>
<li><em>Confidential outcome variables</em>. These are variables which contain sensitive information on the respondent. Examples are salary, religion, political affiliation, health condition, etc.</li>
<li><em>Non-confidential outcome variables</em>. Those variables which do not fall in any of the categories above.</li>
</ul>
<p>The purpose of SDC is to prevent confidential information from being linked to specific respondents. Therefore, we will assume in what follows that original microdata sets to be protected have been pre-processed so as to remove identifiers and quasi-identifiers with low ambiguity (such as name).</p>
<p>The purpose of microdata SDC mentioned in the previous section can be stated more formally by saying that, given an original microdata set <span class="math inline">\(V\)</span>, the goal is to release a protected microdata set <span class="math inline">\(V'\)</span> in such a way that:</p>
<ol type="1">
<li>Disclosure risk (<em>i.e.</em> the risk that a user or an intruder can use <span class="math inline">\(V'\)</span> to determine confidential variables on a specific individual among those in <span class="math inline">\(V\)</span>) is low.</li>
<li>User analyses (regressions, means, etc.) on <span class="math inline">\(V'\)</span> and on <span class="math inline">\(V\)</span> yield the same or at least similar results.</li>
</ol>
<p>Microdata protection methods can generate the protected microdata set <span class="math inline">\(V'\)</span></p>
<ul>
<li>either by <em>masking original data</em>, <em>i.e.</em> generating <span class="math inline">\(V'\)</span> a modified version of the original microdata set <span class="math inline">\(V\)</span>;</li>
<li>or by <em>generating synthetic data</em> <span class="math inline">\(V'\)</span> that preserve some statistical properties of the original data <span class="math inline">\(V\)</span>.</li>
</ul>
<p>Masking methods can in turn be divided in two categories depending on their effect on the original data (Willenborg and DeWaal, 2001):</p>
<ul>
<li><em>Perturbative masking</em>. The microdata set is distorted before publication. In this way, unique combinations of scores in the original dataset may disappear and new unique combinations may appear in the perturbed dataset; such confusion is beneficial for preserving statistical confidentiality. The perturbation method used should be such that statistics computed on the perturbed dataset do not differ significantly from the statistics that would be obtained on the original dataset.</li>
<li><em>Non-perturbative masking</em>. Non-perturbative methods do not alter data; rather, they produce partial suppressions or reductions of detail in the original dataset. Global recoding, local suppression and sampling are examples of non-perturbative masking.</li>
</ul>
<p>At a first glance, synthetic data seem to have the philosophical advantage of circumventing the re-identification problem: since published records are invented and do not derive from any original record, some authors claim that no individual having supplied original data can complain from having been re-identified. At a closer look, some authors (<em>e.g.</em>,&nbsp;Winkler, 2004 and Reiter, 2005) claim that even synthetic data might contain some records that allow for re-identification of confidential information. In short, synthetic data overfitted to original data might lead to disclosure just as original data would. On the other hand, a clear problem of synthetic data is data utility: only the statistical properties explicitly selected by the data protector are preserved, which leads to the question whether the data protector should not directly publish the statistics he wants preserved rather than a synthetic microdata set.</p>
<p>So far in this section, we have classified microdata protection methods by their operating principle. If we consider the type of data on which they can be used, a different dichotomic classification applies:</p>
<ul>
<li><em>Continuous data</em>. A variable is considered continuous if it is numerical and arithmetic operations can be performed with it. Examples are income and age. Note that a numerical variable does not necessarily have an infinite range, as is the case for age. When designing methods to protect continuous data, one has the advantage that arithmetic operations are possible, and the drawback that every combination of numerical values in the original dataset is likely to be unique, which leads to disclosure if no action is taken.</li>
<li><em>Categorical data</em>. A variable is considered categorical when it takes values over a finite set and standard arithmetic operations do not make sense. Ordinal scales and nominal scales can be distinguished among categorical variables. In ordinal scales the order between values is relevant, whereas in nominal scales it is not. In the former case, max and min operations are meaningful while in the latter case only pairwise comparison is possible. The instruction level is an example of ordinal variable, whereas eye colour is an example of nominal variable. In fact, all quasi-identifiers in a microdata set are normally categorical nominal. When designing methods to protect categorical data, the inability to perform arithmetic operations is certainly inconvenient, but the finiteness of the value range is one property that can be successfully exploited.</li>
</ul>
</section>
<section id="perturbative-masking" class="level3">
<h3 class="anchored" data-anchor-id="perturbative-masking">Perturbative masking</h3>
<p>Perturbative statistical disclosure control (SDC) methods allow for the release of the entire microdata set, although perturbed values rather than exact values are released. Not all perturbative methods are designed for continuous data; this distinction is addressed further below for each method.</p>
<p>Most perturbative methods reviewed below (including noise addition, rank swapping, microaggregation and post-randomization) are special cases of matrix masking. If the original microdata set is <span class="math inline">\(X\)</span>, then the masked microdata set <span class="math inline">\(Z\)</span> is computed as</p>
<p><span class="math display">\[
Z=AXB+C
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is a record-transforming mask, <span class="math inline">\(B\)</span> is a variable-transforming mask and <span class="math inline">\(C\)</span> is a displacing mask or noise (Duncan and Pearson, 1991).</p>
<p><a href="#tbl-perturbative-methods">Table&nbsp;1</a> lists the perturbative methods described below. For each method, the table indicates whether it is suitable for continuous and/or categorical data.</p>
<div id="tbl-perturbative-methods" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Perturbative methods vs.&nbsp;data types. “X” denotes applicable and “(X)” denotes applicable with some adaptation.</caption>
<colgroup>
<col style="width: 32%">
<col style="width: 34%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th><em>Method</em></th>
<th style="text-align: center;"><em>Continuous data</em></th>
<th style="text-align: center;"><em>Categorical data</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noise addition<br>
Microaggregation<br>
Rank swapping<br>
Rounding<br>
Resampling<br>
PRAM<br>
MASSC</td>
<td style="text-align: center;">X<br>
X<br>
X<br>
X<br>
X<br>
<br>
</td>
<td style="text-align: center;"><br>
(X)<br>
X<br>
<br>
<br>
X<br>
X<br>
</td>
</tr>
</tbody>
</table>
</div>
<section id="noise-addition" class="level4">
<h4 class="anchored" data-anchor-id="noise-addition">Noise addition</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>The main noise addition algorithms in the literature are:</p>
<ul>
<li>Masking by uncorrelated noise addition</li>
<li>Masking by correlated noise addition</li>
<li>Masking by noise addition and linear transformation</li>
<li>Masking by noise addition and nonlinear transformation (Sullivan, 1989).</li>
</ul>
<p>For more details on specific algorithms, the reader can check Brand (2002).</p>
<p>In practice, only simple noise addition (two first variants) or noise addition with linear transformation are used. When using linear transformations, a decision has to be made whether to reveal to the data user the parameter <span class="math inline">\(c\)</span> determining the transformations to allow for bias adjustment in the case of subpopulations.</p>
<p>With the exception of the not very practical method of Sullivan(1989), noise addition is not suitable to protect categorical data. On the other hand, it is well suited for continuous data for the following reasons:</p>
<ul>
<li>It makes no assumptions on the range of possible values for <span class="math inline">\(V_{i}\)</span> (which may be infinite).</li>
<li>The noise being added is typically continuous and with mean zero, which suits well with continuous original data.</li>
<li>No exact matching is possible with external files. Depending on the amount of noise added, approximate (interval) matching might be possible. More details can be found in Section 3.4.2 below.</li>
</ul>
</blockquote>
</section>
<section id="multiplicative-noise" class="level4">
<h4 class="anchored" data-anchor-id="multiplicative-noise">Multiplicative Noise</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>One main challenge regarding additive noise with constant variance is that on one hand small values are strongly perturbed and on the other large values are weakly perturbed. For instance, in a business microdata set the large enterprises -- which are much easier to re-identify than the smaller ones -- remain still high at risk after noise addition. A possible way out is given by the multiplicative noise approach explained below.</p>
<p>Let <span class="math inline">\(X\)</span> be the matrix of the original data and <span class="math inline">\(W\)</span> the matrix of continuous perturbation variables with expectation 1 and variance <span class="math inline">\(\sigma_{w}^{2} &gt; 0\)</span>. The corresponding anonymised data <span class="math inline">\(X^{a}\)</span> is then obtained as</p>
<p><span class="math display">\[
\left(X^{a}\right)_{ij} : = w_{ij} \cdot X_{ij}
\]</span></p>
<p>for each pair <span class="math inline">\((i,j)\)</span>.</p>
<p>The following approach has been suggested by Höhne (2004). In a first step, for each record it is randomly decided whether its values are increased or decreased, each with 0.5-probability. This is done using the main factors <span class="math inline">\(1 - f\)</span> and <span class="math inline">\(1 + f\)</span>. In order to avoid that all values of some record are perturbed with the same noise, these main factors are themselves perturbed with some additive noise <span class="math inline">\(s\)</span> (where <span class="math inline">\(s &lt; f/2\)</span>). The following transformation is needed to preserve the first and second moments of the distribution:</p>
<p><span class="math display">\[
x_{i}^{a^{R}} : = \frac{ \sigma_{X} }{\sigma_{X^{a}}} \left( x_{i}^{a} - \mu_{X^{a}} \right) + \mu_{X},
\]</span></p>
<p>where <span class="math inline">\(\mu_{X}\)</span> and <span class="math inline">\(\mu_{X^{a}}\)</span> define the average of the original and anonymised variables, <span class="math inline">\(\sigma_{X}\)</span> and <span class="math inline">\(\sigma_{X^{a}}\)</span> the corresponding standard deviations, respectively.</p>
<p>Particularly if the original data follow a strongly skewed distribution, the deviations using this method may strongly depend on the configuration of the noise factors for some few, but large values. That is, despite consistency, means and sums might be unsatisfactorily reproduced. For this reason, (Höhne, 2004) suggests a slight modification of the method. At first, we generate normal distributed random variables <span class="math inline">\(W_{i}\)</span> with expectation greater than zero and 'small' variance, s.t. the realisation of <span class="math inline">\(W_{i}\)</span> yields a positive value. Afterwards, the data is sorted in descending order by the variable under consideration. Then, the record with the largest entry in this variable is diminished by</p>
<p><span class="math display">\[
X_{1}^{a} = \left( 1 - W_{1} \right) X_{1} \quad .
\]</span></p>
<p>The records <span class="math inline">\(X_{2} , \ldots , X_{n-1}\)</span> are now perturbed as follows:</p>
<p><span class="math display">\[
X_{i}^{a} = \begin{cases}
\left( 1 - W_{i} \right) X_{i} , &amp;\text{if}\quad \sum\limits_{k=1}^{i-1}X_{k}^{a} &gt; \sum\limits_{k=1}^{i-1}X_{k}\\
\left( 1 + W_{i}\right) X_{i} , &amp;\text{if}\quad \sum\limits_{k=1}^{i-1}X_{k}^{a} \leq \sum\limits_{k=1}^{i-1}X_{k} \quad .
\end{cases}
\]</span></p>
<p>Hence, means and sums are preserved and the diminishing and enlarging effects of single values cancel out each other. For the remaining record <span class="math inline">\(X_{n}\)</span> we set</p>
<p><span class="math display">\[
X_{n}^{a} = X_{n} - \left( \sum\limits_{k=1}^{n-1} X_{k}^{a} - \sum\limits_{k=1}^{n-1} X_{k} \right)
\]</span></p>
in order to preserve the overall sum.
</blockquote>
</section>
<section id="microaggregation" class="level4">
<h4 class="anchored" data-anchor-id="microaggregation">Microaggregation</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Microaggregation is a family of SDC techniques for continuous microdata. The rationale behind microaggregation is that confidentiality rules in use allow publication of microdata sets if records correspond to groups of <span class="math inline">\(k\)</span> or more individuals, where no individual dominates (<em>i.e.</em> contributes too much to) the group and <span class="math inline">\(k\)</span> is a threshold value. Strict application of such confidentiality rules leads to replacing individual values with values computed on small aggregates (microaggregates) prior to publication. This is the basic principle of microaggregation.</p>
<p>To obtain microaggregates in a microdata set with <span class="math inline">\(n\)</span> records, these are combined to form <span class="math inline">\(g\)</span> groups of size at least <span class="math inline">\(k\)</span>. For each variable, the average value over each group is computed and is used to replace each of the original averaged values. Groups are formed using a criterion of maximal similarity. Once the procedure has been completed, the resulting (modified) records can be published.</p>
<p>Microaggregation exists in several variants:</p>
<ul>
<li>Fixed vs.&nbsp;variable group (Defays and Nanopoulos, 1993), (Mateo-Sanz and Domingo-Ferrer, 1999), (Domingo-Ferrer and Mateo-Sanz, 2002), (Sande, 2002).</li>
<li>Exact optimal vs.&nbsp;heuristic microaggregation (Hansen and Mukherjee, 2003), (Oganian and Domingo-Ferrer, 2001).</li>
<li>Categorical microaggregation (V. Torra, 2004).</li>
</ul>
<br>
More details on the microaggregation implemented in μ‑ARGUS are given below.
</blockquote>
</section>
<section id="data-swapping-and-rank-swapping" class="level4">
<h4 class="anchored" data-anchor-id="data-swapping-and-rank-swapping">Data swapping and rank swapping</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Data swapping was originally presented as an SDC method for databases containing only categorical variables (Dalenius and Reiss, 1978). The basic idea behind the method is to transform a database by exchanging values of confidential variables among individual records. Records are exchanged in such a way that low-order frequency counts or marginals are maintained.</p>
<p>Even though the original procedure was not very used in practice (see Fienberg and McIntyre, 2004), its basic idea had a clear influence in subsequent methods. In Reiss, Post and Dalenius (1982) and Reiss (1984) data swapping was introduced to protect continuous and categorical microdata, respectively. Another variant of data swapping for microdata is <em>rank swapping</em>. Although originally described only for ordinal variables (Greenberg, 1987), rank swapping can also be used for any numerical variable (Moore, 1996). First, values of a variable <span class="math inline">\(X_{i}\)</span> are ranked in ascending order, then each ranked value of <span class="math inline">\(X_{i}\)</span> is swapped with another ranked value randomly chosen within a restricted range (<em>e.g.</em> the rank of two swapped values cannot differ by more than <span class="math inline">\(p\%\)</span> of the total number of records, where <span class="math inline">\(p\)</span> is an input parameter). This algorithm is independently used on each original variable in the original data set.</p>
<p>It is reasonable to expect that multivariate statistics computed from data swapped with this algorithm will be less distorted than those computed after an unconstrained swap. In earlier empirical work by these authors on continuous microdata protection (Domingo-Ferrer and Torra, 2001), rank swapping has been identified as a particularly well-performing method in terms of the tradeoff between disclosure risk and information loss. Consequently, it is one of the techniques that have been implemented in the μ‑ARGUS package (Hundepool et al., 2005).</p>
<p><strong>Example.</strong> In <a href="#tbl-ex-rank-swapping">Table&nbsp;2</a>, we can see an original microdata set on the left and its rankswapped version on the right. There are four variables and ten records in the original dataset; the second variable is alphanumeric, and the standard alphabetic order has been used to rank it. A value of p=10% has been used for all variables.</p>
<div id="tbl-ex-rank-swapping" class="tbl-parent quarto-layout-panel anchored">
<div class="panel-caption table-caption">
<p>Table&nbsp;2: Example of rank swapping.</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-ex-rank-swapping-1" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-ex-rank-swapping" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(a) Original file</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 25%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: center;">K</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">3.4</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: center;">N</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">4.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: center;">M</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">5.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">6.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">7.5</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">10.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: center;">F</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">11.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9</td>
<td style="text-align: center;">D</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">9.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: center;">C</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">3.2</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-ex-rank-swapping-2" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-ex-rank-swapping" style="flex-basis: 50.0%;justify-content: center;">
<table class="table">
<caption>(b) Rankswapped file</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 25%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">4.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">3.2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: center;">M</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">4.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: center;">N</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">6.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: center;">L</td>
<td style="text-align: right;">4.5</td>
<td style="text-align: right;">5.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: center;">F</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">9.5</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: center;">K</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">11.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: center;">H</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">10.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9</td>
<td style="text-align: center;">C</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">7.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td style="text-align: center;">D</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">3.4</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</blockquote>
</section>
<section id="rounding" class="level4">
<h4 class="anchored" data-anchor-id="rounding">Rounding</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Rounding methods replace original values of variables with rounded values. For a given variable <span class="math inline">\(X_{i}\)</span>, rounded values are chosen among a set of rounding points defining a <em>rounding set</em>. In a multivariate original dataset, rounding is usually performed one variable at a time (<em>univariate</em> rounding); however, multivariate rounding is also possible&nbsp;(Willenborg and DeWaal, 2001). The operating principle of rounding makes it suitable for continuous data.</p>
<strong>Example</strong> Assume a non-negative continuous variable <span class="math inline">\(X\)</span>. Then we have to determine a set of rounding points<span class="math inline">\(\left\{ p_0,\cdots,p_r \right\}\)</span>. One possibility is to take rounding points as multiples of a base value <span class="math inline">\(b\)</span>, that is, <span class="math inline">\(p_{i} = b i\)</span> for <span class="math inline">\(i = 1,\cdots,r\)</span>. The set of attraction for each rounding point <span class="math inline">\(p_i\)</span> is defined as the interval <span class="math inline">\(\left\lbrack p_{i} - b/2,p_{i} + b/2 \right)\)</span>, for <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(r - 1\)</span>. For <span class="math inline">\(p_0\)</span> and <span class="math inline">\(p_r\)</span>, respectively, the sets of attraction are <span class="math inline">\(\left\lbrack 0, b/2 \right)\)</span> and <span class="math inline">\(\left\lbrack p_{r} - b/2, X_{\text{max}} \right\rbrack\)</span>, where <span class="math inline">\(X_{\text{max}}\)</span> is the largest possible value for variable <span class="math inline">\(X\)</span>. Now an original value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> is replaced with the rounding point corresponding to the set of attraction where <span class="math inline">\(x\)</span> lies.
</blockquote>
</section>
<section id="resampling" class="level4">
<h4 class="anchored" data-anchor-id="resampling">Resampling</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
Originally proposed for protecting tabular data (Heer, 1993), (Domingo-Ferrer and Mateo-Sanz, 1999), resampling can also be used for microdata. Take <span class="math inline">\(t\)</span> independent samples <span class="math inline">\(S_{1},\cdots,S_{t}\)</span> of the values of an original variable <span class="math inline">\(X_{i}\)</span>. Sort all samples using the same ranking criterion. Build the masked variable <span class="math inline">\(Z_{i}\)</span> as <span class="math inline">\({\overline{x}}_{1},\cdots,{\overline{x}}_{n}\)</span>, where <span class="math inline">\(n\)</span> is the number of records and <span class="math inline">\({\overline{x}}_{j}\)</span> is the average of the <span class="math inline">\(j\)</span>-th ranked values in <span class="math inline">\(S_{1},\cdots,S_{t}\)</span>.
</blockquote>
</section>
<section id="pram" class="level4">
<h4 class="anchored" data-anchor-id="pram">PRAM</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>The Post-RAndomization Method or PRAM (Gouweleeuw et al., 1997) is a probabilistic, perturbative method for disclosure protection of categorical variables in microdata files. In the masked file, the scores on some categorical variables for certain records in the original file are changed to a different score according to a prescribed probability mechanism, namely a Markov matrix. The Markov approach makes PRAM very general, because it encompasses noise addition, data suppression and data recoding.</p>
<p>PRAM information loss and disclosure risk largely depend on the choice of the Markov matrix and are still (open) research topics&nbsp;(DeWolf et al., 1999).</p>
The PRAM matrix contains a row for each possible value of each variable to be protected. This rules out using the method for continuous data. More details on PRAM can be found below.
</blockquote>
</section>
<section id="massc" class="level4">
<h4 class="anchored" data-anchor-id="massc">MASSC</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>MASSC (Singh, Yu and Dunteman, 2003) is a masking method whose acronym summarizes its four steps: Micro Agglomeration, Substitution, Subsampling and Calibration. We briefly recall the purpose of those four steps:</p>
<ol type="1">
<li>Micro agglomeration is applied to partition the original dataset into risk strata (groups of records which are at a similar risk of disclosure). These strata are formed using the key variables, <em>i.e.</em> the quasi-identifiers in the records. The idea is that those records with rarer combinations of key variables are at a higher risk.</li>
<li>Optimal probabilistic substitution is then used to perturb the original data. (i.e.&nbsp;substitution is governed by a Markov matrix like in PRAM, see [Singh, Yu and Dunteman, 2003] for details)</li>
<li>Optimal probabilistic subsampling is used to suppress some variables or even entire records (i.e.&nbsp;variables and/or records are suppressed with a certain probability set as parameters).</li>
<li>Optimal sampling weight calibration is used to preserve estimates for outcome variables in the treated database whose accuracy is critical for the intended data use.</li>
</ol>
<br>
MASSC, to the best of our knowledge, is the first attempt at designing a perturbative masking method in such a way that disclosure risk can be analytically quantified. Its main shortcoming is that its disclosure model simplifies reality by considering only disclosure resulting from linkage of key variables with external sources. Since key variables are typically categorical, the uniqueness approach can be used to analyze the risk of disclosure; however, doing so ignores the fact that continuous outcome variables can also be used for respondent re-identification. As an example, if respondents are companies and turnover is one outcome variable, everyone in a certain industrial sector knows which is the company with largest turnover. Thus, in practice, MASSC is a method only suited when continuous variables are not present.
</blockquote>
</section>
</section>
<section id="non-perturbative-masking" class="level3">
<h3 class="anchored" data-anchor-id="non-perturbative-masking">Non-perturbative masking</h3>
<p>Non-perturbative masking does not rely on distortion of the original data but on partial suppressions or reductions of detail. Some of the methods are usable on both categorical and continuous data, but others are not suitable for continuous data. lists the non-perturbative methods described below. For each method, the <a href="#tbl-non-perturbative-methods">Table&nbsp;3</a> indicates whether it is suitable for continuous and/or categorical data.</p>
<div id="tbl-non-perturbative-methods" class="anchored">
<table class="table">
<caption>Table&nbsp;3: Non-perturbative methods vs.&nbsp;data types.</caption>
<colgroup>
<col style="width: 37%">
<col style="width: 30%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><em>Method</em></th>
<th style="text-align: center;"><em>Continuous data</em></th>
<th style="text-align: center;"><em>Categorical data</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Sampling</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
</tr>
<tr class="even">
<td style="text-align: center;">Global recoding</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Top and bottom coding</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
</tr>
<tr class="even">
<td style="text-align: center;">Local suppression</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">X</td>
</tr>
</tbody>
</table>
</div>
<section id="sampling" class="level4">
<h4 class="anchored" data-anchor-id="sampling">Sampling</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Instead of publishing the original microdata file, what is published is a sample <span class="math inline">\(S\)</span> of the original set of records.</p>
<p>Sampling methods are suitable for categorical microdata, but their adequacy for continuous microdata is less clear in a general disclosure scenario. The reason is that such methods leave a continuous variable <span class="math inline">\(V_{i}\)</span> unperturbed for all records in <span class="math inline">\(S\)</span>. Thus, if variable <span class="math inline">\(V_{i}\)</span> is present in an external administrative public file, unique matches with the published sample are very likely: indeed, given a continuous variable <span class="math inline">\(V_{i}\)</span> and two respondents <span class="math inline">\(o_{1}\)</span> and <span class="math inline">\(o_{2}\)</span>, it is highly unlikely that <span class="math inline">\(V_{i}\)</span> will take the same value for both <span class="math inline">\(o_{1}\)</span> and <span class="math inline">\(o_{2}\)</span> unless <span class="math inline">\(o_{1} = o_{2}\)</span> (this is true even if <span class="math inline">\(V_{i}\)</span> has been truncated to represent it digitally).</p>
<p>If, for a continuous identifying variable, the score of a respondent is only approximately known by an attacker (as assumed in&nbsp;Willenborg and De Waal, 1996) it might still make sense to use sampling methods to protect that variable. However, assumptions on restricted attacker resources are perilous and may prove definitely too optimistic if good quality external administrative files are at hand. For the purpose of illustration, the example&nbsp; below gives the technical specifications of a real-world application of sampling.</p>
<p><strong>Example</strong> Statistics Catalonia released in 1995 a sample of the 1991 population census of Catalonia. The information released corresponds to 36 categorical variables (including the recoded versions of initially continuous variables); some of the variables are related to the individual person and some to the household. The technical specifications of the sample were as follows:</p>
<ul>
<li>Sampling algorithm: Simple random sampling.</li>
<li>Sampling unit: Individuals in the population whose residence was in Catalonia as of March 1, 1991.</li>
<li>Population size: 6,059,494 inhabitants</li>
<li>Sample size: 245,944 individual records</li>
<li>Sampling fraction: <em>0.0406</em></li>
</ul>
<br>
With the above sampling fraction, the maximum absolute error for estimating a maximum-variance proportion is 0.2 percent.
</blockquote>
</section>
<section id="global-recoding" class="level4">
<h4 class="anchored" data-anchor-id="global-recoding">Global recoding</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>For a categorical variable <span class="math inline">\(V_{i}\)</span>, several categories are combined to form new (less specific) categories, thus resulting in a new <span class="math inline">\(V_{i}'\)</span> with <span class="math inline">\(\left| D\left( V_{i}' \right) \right| &lt; \left| D\left( V_{i} \right) \right|\)</span> where <span class="math inline">\(|\cdot |\)</span> is the cardinality operator and <span class="math inline">\(D(V_i)\)</span> denotes the domain of variable <span class="math inline">\(V_i\)</span>, <em>i.e.</em>, the possible values <span class="math inline">\(V_i\)</span> can have. For a continuous variable, global recoding means replacing <span class="math inline">\(V_{i}\)</span> by another variable <span class="math inline">\(V_{i}'\)</span> which is a discretized version of <span class="math inline">\(V_{i}\)</span>. In other words, a potentially infinite range <span class="math inline">\(D\left( V_{i} \right)\)</span> is mapped onto a finite range <span class="math inline">\(D\left( V_{i}' \right)\)</span>. This is the technique used in the μ‑ARGUS SDC package&nbsp;(Hundepool et al.&nbsp;2005).</p>
<p>This technique is more appropriate for categorical microdata, where it helps disguise records with strange combinations of categorical variables. Global recoding is used heavily by statistical offices.</p>
<p><strong>Example.</strong> If there is a record with “Marital status = Widow/er” and “Age = 17”, global recoding could be applied to “Marital status” to create a broader category “Widow/er or divorced”, so that the probability of the above record being unique would diminish. Global recoding can also be used on a continuous variable, but the inherent discretization leads very often to an unaffordable loss of information. Also, arithmetical operations that were straightforward on the original <span class="math inline">\(V_{i}\)</span> are no longer easy or intuitive on the discretized <span class="math inline">\(V_{i}'\)</span>.</p>
<p><strong>Example.</strong> We can recode the variable ‘Occupation’, by combining the categories ‘Statistician’ and ‘Mathematician’ into a single category ‘Statistician or Mathematician’. When the number of female statisticians in Urk (a small town) plus the number of female mathematicians in Urk is sufficiently high, then the combination ‘Place of residence = Urk’, ‘Gender = Female’ and ‘Occupation = Statistician or Mathematician’ is considered safe for release. Note that instead of recoding ‘Occupation’ one could also recode ‘Place of residence’ for instance.</p>
It is important to realise that global recoding is applied to the whole data set, not only to the unsafe part of the set. This is done to obtain a uniform categorisation of each variable. Suppose, for instance, that we recode the ‘Occupation’ in the above way. Suppose furthermore that both the combinations ‘Place of residence = Amsterdam’, ‘Gender = Female’ and ‘Occupation = Statistician’, and ‘Place of residence = Amsterdam’, ‘Gender = Female’ and ‘Occupation = Mathematician’ are considered safe. To obtain a uniform categorisation of ‘Occupation’ we would, however, not publish these combinations, but only the combination ‘Place of residence = Amsterdam’, ‘Gender = Female’ and ‘Occupation = Statistician or Mathematician’.
</blockquote>
</section>
<section id="top-and-bottom-coding" class="level4">
<h4 class="anchored" data-anchor-id="top-and-bottom-coding">Top and bottom coding</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
Top and bottom coding is a special case of global recoding which can be used on variables that can be ranked, that is, continuous or categorical ordinal. The idea is that top values (those above a certain threshold) are lumped together to form a new category. The same is done for bottom values (those below a certain threshold). See&nbsp;the μ‑ARGUS manual (Hundepool et al.&nbsp;2005).
</blockquote>
</section>
<section id="local-suppression" class="level4">
<h4 class="anchored" data-anchor-id="local-suppression">Local suppression</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Certain values of individual variables are suppressed with the aim of increasing the set of records agreeing on a combination of key values. Ways to combine local suppression and global recoding are discussed in (De Waal and Willenborg, 1995) and implemented in the μ‑ARGUS SDC package&nbsp;(Hundepool et al.&nbsp;2005).</p>
<p>If a continuous variable <span class="math inline">\(V_{i}\)</span> is part of a set of key variables, then each combination of key values is probably unique. Since it does not make sense to systematically suppress the values of <span class="math inline">\(V_{i}\)</span>, we conclude that local suppression is rather oriented to categorical variables.</p>
When local suppression is applied, one or more values in an unsafe combination are suppressed, i.e.&nbsp;replaced by a missing value. For instance, in the above example we can protect the unsafe combination ‘Place of residence = Urk’, ‘Gender = Female’ and ‘Occupation = Statistician’ by suppressing the value of ‘Occupation’, assuming that the number of females in Urk is sufficiently high. The resulting combination is then given by ‘Place of residence = Urk’, ‘Gender = Female’ and ‘Occupation = missing’. Note that instead of suppressing the value of ‘Occupation’ one could also suppress the value of another variable of the unsafe combination. For instance, when the number of female statisticians in the Netherlands is sufficiently high then one could suppress the value of ‘Place of residence’ instead of the value of ‘Occupation’ in the above example to protect the unsafe combination. A local suppression is only applied to a particular value. When, for instance, the value of ‘Occupation’ is suppressed in a particular record, then this does not imply that the value of ‘Occupation’ has to be suppressed in another record. The freedom that one has in selecting the values that are to be suppressed allows one to minimise the number of local suppressions.
</blockquote>
</section>
<section id="references" class="level4">
<h4 class="anchored" data-anchor-id="references">References</h4>
<p>Brand, R. (2002<em>). Microdata protection through noise addition.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 97–116, Berlin Heidelberg, 2002. Springer.</p>
<p>Dalenius T., and Reiss, S. P. (1978). <em>Data-swapping: a technique for disclosure control</em> (extended abstract). In Proc. of the ASA Section on Survey Research Methods, pages 191–194, Washington DC, 1978. American Statistical Association.</p>
<p><em>Defays, D., and Nanopoulos, P. (1993). Panels of enterprises and confidentiality: the small aggregates method</em>. In Proc. of 92 Symposium on Design and Analysis of Longitudinal Surveys, pages 195–204, Ottawa, 1993. Statistics Canada.</p>
<p>De Waal, A. G., and Willenborg, L.C.R.J. (1995). <em>Global recodings and local suppressions in microdata sets</em>. In Proceedings of Statistics Canada Symposium’95, pages 121–132, Ottawa, 1995. Statistics Canada.</p>
<p>De Waal, A.G. and Willenborg, L. C. R. J. (1999). <em>Information loss through global recoding and local suppression</em>. Netherlands Official Statistics, 14:17–20, 1999. special issue on SDC.</p>
<p>De Wolf, P.-P., Gouweleeuw, J. M., Kooiman, P., and Willenborg, L.C.R.J. (1999). <em>Reflections on PRAM</em>. In J.&nbsp;Domingo-Ferrer, editor, Statistical Data Protection, pages 337–349, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Domingo-Ferrer, J., and Mateo-Sanz, J. M. (1999). <em>On resampling for statistical confidentiality in contingency tables.</em> Computers &amp; Mathematics with Applications, 38:13–32, 1999.</p>
<p>Domingo-Ferrer, J., and Mateo-Sanz, J. M. (2002). <em>Practical data-oriented microaggregation for statistical disclosure control</em>. IEEE Transactions on Knowledge and Data Engineering, 14(1):189–201, 2002.</p>
<p>Domingo-Ferrer, J., Mateo-Sanz, J. M., and Torra, V. (2001). <em>Comparing sdc methods for microdata on the basis of information loss and disclosure risk.</em> In Pre-proceedings of ETK-NTTS’2001 (vol.&nbsp;2), pages 807–826, Luxemburg, 2001. Eurostat.</p>
<p>Domingo-Ferrer, J., and Torra, V., (2001<em>). Disclosure protection methods and information loss for microdata</em>. In P.&nbsp;Doyle, J.&nbsp;I. Lane, J.&nbsp;J.&nbsp;M. Theeuwes, and L.&nbsp;Zayatz, editors, Confidentiality, Disclosure and Data Access: Theory and Practical Applications for Statistical Agencies, pages 91–110, Amsterdam, 2001. North-Holland. <a href="http://vneumann.etse.urv.es/publications/bcpi"><u>http://vneumann.etse.urv.es/publications/bcpi</u></a>.</p>
<p>Duncan, G. T., and Pearson, R. W. (1991). <em>Enhancing access to microdata while protecting confidentiality: prospects for the future</em>. Statistical Science, 6:219–239, 1991.</p>
<p>Fienberg, S. E., and McIntyre, J. (2004). <em>Data swapping: variations on a theme by dalenius and reiss.</em> In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 14–29, Berlin Heidelberg, 2004. Springer.</p>
<p>Gouweleeuw, J. M., Kooiman, P., Willenborg, L. C. R. J., and De Wolf, P.-P. (1997<em>). Post randomisation for statistical disclosure control: Theory and implementation</em>, Research paper no. 9731 (Voorburg: Statistics Netherlands).</p>
<p>Greenberg, B. (1987). <em>Rank swapping for ordinal data</em>, Washington, DC: U. S. Bureau of the Census (unpublished manuscript).</p>
<p>Hansen, S. L., and Mukherjee, S. (2003). <em>A polynomial algorithm for optimal univariate microaggregation</em>. IEEE Transactions on Knowledge and Data Engineering, 15(4):1043–1044, 2003.</p>
<p>Heer, G. R. (1993). <em>A bootstrap procedure to preserve statistical confidentiality in contingency tables</em>. In D.&nbsp;Lievesley, editor, Proc. of the International Seminar on Statistical Confidentiality, pages 261–271, Luxemburg, 1993. Office for Official Publications of the European Communities.</p>
<p>Höhne (2004), Varianten von Zufallsüberlagerung (German), working paper of the project group 'De facto anonymisation of business microdata', Wiesbaden.</p>
<p>Hundepool, A., Van de&nbsp;Wetering, A., Ramaswamy, R., Franconi, L., Capobianchi, A., De Wolf, P.-P., Domingo-Ferrer, J., Torra, V. and Giessing, S. (2005). <em>µ-ARGUS version 4.0 Software and User’s Manual</em>. Statistics Netherlands, Voorburg NL, may 2005. <a href="https://research.cbs.nl/casc/deliv/MUmanual4.0.pdf"><u>https://research.cbs.nl/casc</u></a>.</p>
<p>Kooiman, P. L, Willenborg, L, and Gouweleeuw, J. M. (1998). <em>PRAM: A method for disclosure limitation of microdata</em>. Technical report, Statistics Netherlands (Voorburg, NL), 1998.</p>
<p>Mateo-Sanz, J. M., and Domingo-Ferrer, J. (1999) . <em>A method for data-oriented multivariate microaggregation</em>. In J.&nbsp;Domingo-Ferrer, editor, Statistical Data Protection, pages 89–99, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Moore, R. (1996). <em>Controlled data swapping techniques for masking public use microdata sets, 1996</em>. U. S. Bureau of the Census, Washington, DC, (unpublished manuscript).</p>
<p>Oganian, A., and Domingo-Ferrer, J. (2001). <em>On the complexity of optimal microaggregation for statistical disclosure control</em>. Statistical Journal of the United Nations Economic Commissions for Europe, 18(4):345–354, 2001.</p>
<p>Reiss, S. P, (1984). <em>Practical data-swapping: the first steps</em>. ACM Transactions on Database Systems, 9:20–37, 1984.</p>
<p>Reiss, S. P., Post, M. J., and Dalenius, T. (1982). <em>Non-reversible privacy transformations</em>. In Proceedings of the ACM Symposium on Principles of Database Systems, pages 139–146, Los Angeles, CA, 1982. ACM.</p>
<p>Reiter, J. P. (2005). <em>Releasing multiply-imputed, synthetic public use microdata: An illustration and empirical study.</em> Journal of the Royal Statistical Society, Series A, 168:185–205, 2005.</p>
<p>Sande, G. (2002). <em>Exact and approximate methods for data directed microaggregation in one or more dimensions</em>. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):459–476, 2002.</p>
<p>Singh, A. C, Yu, F., and Dunteman, G. H. (2003) <em>. Massc: A new data mask for limiting statistical information loss and disclosure</em>. In H.&nbsp;Linden, J.&nbsp;Riecan, and L.&nbsp;Belsby, editors, Work Session on Statistical Data Confidentiality 2003, Monographs in Official Statistics, pages 373–394, Luxemburg, 2004. Eurostat.</p>
<p>Sullivan, G. R. (1989). <em>The Use of Added Error to Avoid Disclosure in Microdata Releases</em>. PhD thesis, Iowa State University, 1989.</p>
<p>Torra, V. (2004). <em>Microaggregation for categorical variables: a median based approach</em>. In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 162–174, Berlin Heidelberg, 2004. Springer.</p>
<p>Willenborg, L. and De Waal, T. (1996) . <em>Statistical Disclosure Control in Practice</em>. Springer-Verlag, New York, 1996.</p>
<p>Willenborg, L., and De Waal, T. (2001). <em>Elements of Statistical Disclosure Control.</em> Springer-Verlag, New York, 2001.</p>
<p>Winkler, W. E. (2004). <em>Re-identification methods for masked microdata.</em> In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 216–230, Berlin Heidelberg, 2004. Springer.</p>
</section>
</section>
<section id="noise-addition-1" class="level3">
<h3 class="anchored" data-anchor-id="noise-addition-1">Noise addition</h3>
<!-- **SECTION 3.3.2 HÖHNE'S METHOD TO BE ADDED BY SARAH AT THE END OF THIS SECTION (remark by Josep)** -->
<p>We sketch in this Section the operation of the main noise addition algorithms in the literature for microdata protection. For more details on specific algorithms, the reader can check (Brand, 2002).</p>
<section id="masking-by-uncorrelated-noise-addition" class="level4">
<h4 class="anchored" data-anchor-id="masking-by-uncorrelated-noise-addition">Masking by uncorrelated noise addition</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Masking by additive noise assumes that the vector of observations <span class="math inline">\(x_{j}\)</span> for the <em>j</em>-th variable of the original dataset <span class="math inline">\(X_{j}\)</span> is replaced by a vector where <span class="math inline">\(\varepsilon_{j}\)</span> is a vector of normally distributed errors drawn from a random variable <span class="math inline">\(\varepsilon_{j} \sim N\left( 0,\sigma_{\varepsilon_{j}}^{2} \right)\)</span>, such that <span class="math inline">\(\text{Cov}\left( \varepsilon_{t},\varepsilon_{l} \right)=0\)</span> for all <span class="math inline">\(t \neq l\)</span> (white noise).</p>
<p>The general assumption in the literature is that the variances of the <span class="math inline">\(\varepsilon_{j}\)</span> are proportional to those of the original variables. Thus, if <span class="math inline">\(\sigma_{j}^{2}\)</span> is the variance of <span class="math inline">\(X_{j}\)</span>, then <span class="math inline">\(\sigma_{\varepsilon_{j}}^{2}: = \alpha\sigma_{j}^{2}\)</span>.</p>
<p>In the case of a <span class="math inline">\(p\)</span>-dimensional dataset, simple additive noise masking can be written in matrix notation as <span class="math inline">\(Z=X + \epsilon\)</span>, where <span class="math inline">\(X \sim (\mu,\Sigma)\)</span>, <span class="math inline">\(\varepsilon \sim \left( 0,\Sigma_{\varepsilon} \right)\)</span> and</p>
<p><span class="math inline">\(\Sigma_{\varepsilon} = \alpha \cdot \text{diag}\left( \sigma_{1}^{2},\sigma_{2}^{2},\cdots,\sigma_{p}^{2} \right)\)</span>, for <span class="math inline">\(\alpha &gt; 0\)</span></p>
<p>This method preserves means and covariances, <em>i.e.</em></p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}(Z) &amp;= \mathbb{E}(X) + \mathbb{E}(\epsilon) = \mathbb{E}(X) = \mu\\
\operatorname{Cov}(Z_j,Z_l) &amp;= \operatorname{Cov}(X_j,X_l) \quad \forall j\neq l
\end{align}
\]</span></p>
<p>Unfortunately, neither variances nor correlation coefficients are preserved:</p>
<p><span class="math display">\[
\operatorname{Var}\left( Z_{j} \right) = \operatorname{Var}\left( X_{j} \right) + \alpha\operatorname{Var}\left( X_{j} \right) = (1 + \alpha)\operatorname{Var}\left( X_{j} \right)
\]</span></p>
<span class="math display">\[
\rho(Z_j, Z_l)=\frac{\operatorname{Cov}(Z_j, Z_l)}{\sqrt{\operatorname{Var}(X_j)\operatorname{Var}(X_l)}} = \frac{1}{1+\alpha} \rho(X_j, X_l),\forall j \neq l
\]</span>
</blockquote>
</section>
<section id="masking-by-correlated-noise-addition" class="level4">
<h4 class="anchored" data-anchor-id="masking-by-correlated-noise-addition">Masking by correlated noise addition</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Correlated noise addition also preserves means and additionally allows preservation of correlation coefficients. The difference with the previous method is that the covariance matrix of the errors is now proportional to the covariance matrix of the original data, <em>i.e.</em> <span class="math inline">\(\varepsilon \sim (0,\Sigma)\)</span>, where <span class="math inline">\(\Sigma_{\varepsilon} = \alpha\Sigma\)</span>.</p>
<p>With this method, we have that the covariance matrix of the masked data is</p>
<p><span class="math display">\[
\Sigma_{z} = \Sigma + \alpha\Sigma = (1 + \alpha)\Sigma \quad . \quad \text{(3.3.2.1)}
\]</span></p>
<p>Preservation of correlation coefficients follows, since</p>
<p><span class="math display">\[
\rho (Z_j, Z_l) = \frac{1 + \alpha}{1 + \alpha}\frac{\operatorname{Cov}\left( X_{j},X_{l} \right)}{\sqrt{\operatorname{Var}\left( X_{j} \right)\operatorname{Var}\left( X_{l} \right)}} = \rho(X_{j},X_{l})
\]</span></p>
<p>Regarding variances and covariances, we can see from Equation (3.3.2.1) that masked data only provide biased estimates for them. However, it is shown in Kim (1990) that the covariance matrix of the original data can be consistently estimated from the masked data as long as <span class="math inline">\(\alpha\)</span> is known.</p>
As a summary, masking by correlated noise addition outputs masked data with higher analytical validity than masking by uncorrelated noise addition. Consistent estimators for several important statistics can be obtained as long as α is revealed to the data user. However, simple noise addition as discussed in this section and in the previous one is seldom used because of the very low level of protection it provides&nbsp;(Tendick, 1991), (Tendick and Matloff, 1994).
</blockquote>
</section>
<section id="masking-by-noise-addition-and-linear-transformations" class="level4">
<h4 class="anchored" data-anchor-id="masking-by-noise-addition-and-linear-transformations">Masking by noise addition and linear transformations</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>In&nbsp;Kim (1986), a method is proposed that ensures by additional transformations that the sample covariance matrix of the masked variables is an unbiased estimator for the covariance matrix of the original variables. The idea is to use simple additive noise on the <span class="math inline">\(p\)</span> original variables to obtain overlayed variables</p>
<p><span class="math display">\[
Z_{j} = X_{j} + \varepsilon_{j},\quad \text{for } j = 1,\ldots,p
\]</span> As in the previous section on correlated masking, the covariances of the errors <span class="math inline">\(\varepsilon_{j}\)</span> are taken proportional to those of the original variables. Usually, the distribution of errors is chosen to be normal or the distribution of the original variables, although in&nbsp;Roque (2000) mixtures of multivariate normal noise are proposed.</p>
<p>In a second step, every overlayed variable <span class="math inline">\(Z_{j}\)</span> is transformed into a masked variable <span class="math inline">\(G_{j}\)</span> as</p>
<p><span class="math display">\[
G_{j} = cZ_{j} + d_{j}
\]</span></p>
<p>In matrix notation, this yields</p>
<p><span class="math display">\[
Z = X + \varepsilon
\]</span></p>
<p><span class="math display">\[
G = cZ_{j} + D = c(X + \varepsilon) + D
\]</span></p>
<p>where <span class="math inline">\(X \sim N(\mu,\Sigma),\varepsilon \sim \left( 0,\alpha\Sigma \right),G \sim (\mu,\Sigma)\)</span> and <span class="math inline">\(D\)</span> is a matrix whose <span class="math inline">\(j\)</span>-th column contains the scalar <span class="math inline">\(d_{j}\)</span> in all rows. Parameters <span class="math inline">\(c\)</span> and <span class="math inline">\(d_{j}\)</span> are determined under the restrictions that <span class="math inline">\(\mathbb{E}\left( G_{j} \right) = \mathbb{E}\left( X_{j} \right)\)</span> and <span class="math inline">\(\operatorname{Var}\left( G_{j} \right) = \operatorname{Var}\left( X_{j} \right)\)</span> for <span class="math inline">\(j = 1,\cdots,p\)</span>. In fact, the first restriction implies that <span class="math inline">\(d_{j} = (1 - c)\mathbb{E}\left( X_{j} \right)\)</span>, so that the linear transformations depend on a single parameter <span class="math inline">\(c\)</span>.</p>
<p>Due to the restrictions used to determine <span class="math inline">\(c\)</span>, this methods preserves expected values and covariances of the original variables and is quite good in terms of analytical validity. Regarding analysis of regression estimates in subpopulations, it is shown in&nbsp;Kim (1990) that (masked) sample means and covariances are asymptotically biased estimates of the corresponding statistics on the original subpopulations. The magnitude of the bias depends on the parameter <span class="math inline">\(c\)</span>, so that estimates can be adjusted by the data user as long as <span class="math inline">\(c\)</span> is revealed to her —revealing <span class="math inline">\(c\)</span> to the user has a fundamental disadvantage, though: the user can undo the linear transformation, so that this method becomes equivalent to plain uncorrelated noise addition&nbsp;(Domingo-Ferrer, Sebé, and Castellà, 2004)</p>
The most prominent shortcomings of this method are that it does not preserve the univariate distributions of the original data and that it cannot be applied to discrete variables due to the structure of the transformations.
</blockquote>
</section>
<section id="masking-by-noise-addition-and-nonlinear-transformations" class="level4">
<h4 class="anchored" data-anchor-id="masking-by-noise-addition-and-nonlinear-transformations">Masking by noise addition and nonlinear transformations</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>An algorithm combining simple additive noise and nonlinear transformation is proposed in Sullivan (1989). The advantages of this proposal are that it can be applied to discrete variables and that univariate distributions are preserved.</p>
<p>The method consists of several steps:</p>
<ol type="1">
<li>Calculate the empirical distribution function for every original variable.</li>
<li>Smooth the empirical distribution function.</li>
<li>Convert the smoothed empirical distribution function into a uniform random variable and this into a standard normal random variable.</li>
<li>Add noise to the standard normal variable.</li>
<li>Back-transform to values of the distribution function.</li>
<li>Back-transform to the original scale.</li>
</ol>
In the European project CASC (IST-2000-25069), the practicality and usability of this algorithm was assessed. Unfortunately, the internal CASC report by&nbsp;Brand (2002b) concluded that:<br>
<em>“All in all, the results indicate that an algorithm as complex as the one proposed by Sullivan can only be applied by experts. Every application is very time-consuming and requires expert knowledge on the data and the algorithm.”</em>
</blockquote>
</section>
<section id="summary-on-noise-addition" class="level4">
<h4 class="anchored" data-anchor-id="summary-on-noise-addition">Summary on noise addition</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Thus, in practice, only simple noise addition or noise addition with linear transformation are used. When using linear transformations, a decision has to be made whether to reveal to the data user the parameter <span class="math inline">\(c\)</span> determining the transformations to allow for bias adjustment in the case of subpopulations.</p>
<p>With the exception of the not very practical method of&nbsp;Sullivan (1989), additive noise is not suitable to protect categorical data. On the other hand, it is well suited for continuous data for the following reasons:</p>
<ul>
<li>It makes no assumptions on the range of possible values for <span class="math inline">\(V_{i}\)</span> (which may be infinite).</li>
<li>The noise being added is typically continuous and with mean zero, which suits well continuous original data.</li>
<li>No exact matching is possible with external files. Depending on the amount of noise added, approximate (interval) matching might be possible.</li>
</ul>
</blockquote>
</section>
<section id="references-1" class="level4">
<h4 class="anchored" data-anchor-id="references-1">References</h4>
<p>Brand, R. (2002). <em>Microdata protection through noise addition</em>. In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 97–116, Berlin Heidelberg, 2002. Springer.</p>
<p>Brand, R. (2002b). <em>Tests of the applicability of sullivan’s algorithm to synthetic data and real business data in official statistics,</em> European Project IST-2000-25069 CASC, Deliverable 1.1-D1, <a href="https://research.cbs.nl/casc/deliv/11d1.pdf"><u>https://research.cbs.nl/casc</u></a>.</p>
<p>Domingo-Ferrer, J., Sebé, F., and Castellà, J. (2004). <em>On the security of noise addition for privacy in statistical databases.</em> In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 149–161, Berlin Heidelberg, 2004. Springer.</p>
<p>Kim, J. J. (1986). <em>A method for limiting disclosure in microdata based on random noise and transformation.</em> In Proceedings of the Section on Survey Research Methods, pages 303–308, Alexandria VA, American Statistical Association.</p>
<p>Kim, J. J. (1990). <em>Subpopulation estimation for the masked data.</em> In Proceedings of the ASA Section on Survey Research Methods, pages 456–461, Alexandria VA, 1990. American Statistical Association.</p>
<p>Roque, G. M. (2000).. <em>Masking Microdata Files with Mixtures of Multivariate Normal Distributions</em>. PhD thesis, University of California at Riverside, 2000.</p>
<p>Sullivan, G. R. (1989<em>). The Use of Added Error to Avoid Disclosure in Microdata Releases</em>. PhD thesis, Iowa State University.</p>
<p>Tendick, P. (1991). <em>Optimal noise addition for preserving confidentiality in multivariate data.</em> Journal of Statistical Planning and Inference, 27:341–353, 1991.</p>
<p>Tendick, P., and Matloff, N. (1994). <em>A modified random perturbation method for database security</em>. ACM Transactions on Database Systems, 19:47–63.</p>
</section>
</section>
<section id="microaggregation-further-details" class="level3">
<h3 class="anchored" data-anchor-id="microaggregation-further-details">Microaggregation: further details</h3>
<p>Consider a microdata set with <span class="math inline">\(p\)</span> continuous variables and <span class="math inline">\(n\)</span> records (<em>i.e.</em>, the result of recording <span class="math inline">\(p\)</span> variables on <span class="math inline">\(n\)</span> individuals). A particular record can be viewed as an instance of <span class="math inline">\(X' = \left( X_{1},\cdots,X_{p} \right)\)</span>, where the <span class="math inline">\(X_{i}\)</span> are the variables. With these individuals, <span class="math inline">\(g\)</span> groups are formed with <span class="math inline">\(n_{i}\)</span> individuals in the <span class="math inline">\(i\)</span>-th group (<span class="math inline">\(n_{i} \geq k\)</span> and <span class="math inline">\(n = \Sigma_{}^{}n_{i}\)</span>). Denote by <span class="math inline">\(x_{\text{ij}}\)</span> the <span class="math inline">\(j\)</span>-th record in the <span class="math inline">\(i\)</span>-th group; denote by <span class="math inline">\({\overline{x}}_{i}\)</span> the average record over the <span class="math inline">\(i\)</span>-th group, and by <span class="math inline">\(\overline{x}\)</span> the average record over the whole set of <span class="math inline">\(n\)</span> individuals.</p>
<p>The optimal <span class="math inline">\(k\)</span>-partition (from the information loss point of view) is defined to be the one that maximizes within-group homogeneity; the higher the within-group homogeneity, the lower the information loss, since microaggregation replaces values in a group by the group centroid. The sum of squares criterion is common to measure homogeneity in clustering. The within-groups sum of squares <span class="math inline">\(\text{SSE}\)</span> is defined as</p>
<p><span class="math display">\[
\text{SSE} = \sum\limits_{i = 1}^{g}\sum\limits_{j = 1}^{n_{i}}\left( x_{\text{ij}} - {\overline{x}}_{i} \right)^{T}\left( x_{\text{ij}} - {\overline{x}}_{i} \right)
\]</span></p>
<p>The lower <span class="math inline">\(\text{SSE}\)</span>, the higher the within group homogeneity. The total sum of squares is</p>
<p><span class="math display">\[
\text{SST} = \sum_{i = 1}^{g}\sum_{j = 1}^{n_{i}}\left( x_{\text{ij}} - \overline{x} \right)^{T}\left( x_{\text{ij}} - \overline{x} \right)
\]</span></p>
<p>In terms of sums of squares, the optimal <span class="math inline">\(k\)</span>-partition is the one that minimizes SSE.</p>
<p>For a microdata set consisting of <span class="math inline">\(p\)</span> variables, these can be microaggregated together or partitioned into several groups of variables. Also the way to form groups may vary. We next review the main proposals in the literature.</p>
<p><strong>Example.</strong> This example illustrates the use of microaggregation for SDC and, more specifically, for <span class="math inline">\(k\)</span>-anonymization&nbsp;(Samarati and L.&nbsp;Sweeney, 1998), (Samarati, 2001), (Sweeney, 2002), (Domingo-Ferrer and Torra, 2005). A <span class="math inline">\(k\)</span>-anonymous dataset allows no re-identification of a respondent within a group of at least <span class="math inline">\(k\)</span>respondents. We show in <a href="#tbl-ex-sme-dataset">Table&nbsp;4</a> a dataset giving, for 11 small or medium enterprises (SMEs) in a certain town, the company name, the surface in square meters of the company’s premises, its number of employees, its turnover and its net profit. Clearly, the company name is an identifier. We will consider that turnover and net profit are confidential outcome variables. A first SDC measure is to suppress the identifier “Company name” when releasing the dataset for public use. However, note that the surface of the company’s premises and its number of employees can be used by a snooper as key variables. Indeed, it is easy for anybody to gauge to a sufficient accuracy the surface and number of employees of a target SME. Therefore, if the only privacy measure taken when releasing the dataset in <a href="#tbl-ex-sme-dataset">Table&nbsp;4</a> is to suppress the company name, a snooper knowing that company K&amp;K Sarl has about a dozen employees crammed in a small flat of about 50 m will still be able to use the released data to link company K&amp;K Sarl with turnover 645,223 Euros and net profit 333,010 Euros. <a href="#tbl-ex-three-anonym-sme">Table&nbsp;5</a> is a 3-anonymous version of the dataset in <a href="#tbl-ex-sme-dataset">Table&nbsp;4</a>. The identifier “Company name” was suppressed and optimal bivariate microaggregation with <span class="math inline">\(k = 3\)</span> was used on the key variables “Surface” and “No.&nbsp;employees” (in general, if there are <span class="math inline">\(p\)</span> key variables, multivariate microaggregation with dimension <span class="math inline">\(p\)</span> should be used to mask all of them). Both variables were standardized to have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span> before microaggregation, in order to give them equal weight, regardless of their scale. Due to the small size of the dataset, it was feasible to compute optimal microaggregation by exhaustive search. The information or variability loss incurred for those two variables in standardized form can be measured by the within-groups sum of squares . Dividing by the total sum of squares SST=22 —sum of squared Euclidean distances from all 11 pairs of standardized (surface, number of employees) to their average— yielded a variability loss measure <span class="math inline">\(SSE_{opt}/SST=0.34\)</span> bounded between 0 and 1.</p>
<p><em>It can be seen that the 11 records were microaggregated into three groups: one group with the 1st, 2nd, 3rd and 10th records (companies with large surface and many employees), a second group with the 4th, 5th and 9th records (companies with large surface and few employees) and a third group with the 6th, 7th, 8th and 11th records (companies with a small surface). Upon seeing <a href="#tbl-ex-three-anonym-sme">Table&nbsp;5</a>, a snooper knowing that company K&amp;K Sarl crams a dozen employees in a small flat hesitates between the four records in the third group. Therefore, since turnover and net profit are different for all records in the third group, the snooper cannot be sure about their values for K&amp;K Sarl.</em></p>
<div id="tbl-ex-sme-dataset" class="anchored">
<table class="table">
<caption>Table&nbsp;4: Example - SME dataset. “Company name” is an identifier to be suppressed before publishing the dataset.</caption>
<colgroup>
<col style="width: 18%">
<col style="width: 17%">
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><strong><em>Company name</em></strong></th>
<th style="text-align: right;"><strong><em>Surface (m2)</em></strong></th>
<th style="text-align: right;"><strong><em>No.&nbsp;employees</em></strong></th>
<th style="text-align: right;"><strong><em>Turnover (Euros)</em></strong></th>
<th style="text-align: right;"><strong><em>Net profit (Euros)</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A&amp;A Ltd</td>
<td style="text-align: right;">790</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">3,212,334</td>
<td style="text-align: right;">313,250</td>
</tr>
<tr class="even">
<td style="text-align: center;">B&amp;B SpA</td>
<td style="text-align: right;">710</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">2,283,340</td>
<td style="text-align: right;">299,876</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C&amp;C Inc</td>
<td style="text-align: right;">730</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">1,989,233</td>
<td style="text-align: right;">200,213</td>
</tr>
<tr class="even">
<td style="text-align: center;">D&amp;D BV</td>
<td style="text-align: right;">810</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">984,983</td>
<td style="text-align: right;">143,211</td>
</tr>
<tr class="odd">
<td style="text-align: center;">E&amp;E SL</td>
<td style="text-align: right;">950</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">194,232</td>
<td style="text-align: right;">51,233</td>
</tr>
<tr class="even">
<td style="text-align: center;">F&amp;F GmbH</td>
<td style="text-align: right;">510</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">119,332</td>
<td style="text-align: right;">20,333</td>
</tr>
<tr class="odd">
<td style="text-align: center;">G&amp;G AG</td>
<td style="text-align: right;">400</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">3,012,444</td>
<td style="text-align: right;">501,233</td>
</tr>
<tr class="even">
<td style="text-align: center;">H&amp;H SA</td>
<td style="text-align: right;">330</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">4,233,312</td>
<td style="text-align: right;">777,882</td>
</tr>
<tr class="odd">
<td style="text-align: center;">I&amp;I LLC</td>
<td style="text-align: right;">510</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">159,999</td>
<td style="text-align: right;">60,388</td>
</tr>
<tr class="even">
<td style="text-align: center;">J&amp;J Co</td>
<td style="text-align: right;">760</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">5,333,442</td>
<td style="text-align: right;">1,001,233</td>
</tr>
<tr class="odd">
<td style="text-align: center;">K&amp;K Sarl</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">645,223</td>
<td style="text-align: right;">333,010</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-ex-three-anonym-sme" class="anchored">
<table class="table">
<caption>Table&nbsp;5: Example - 3-anonymous version of the SME dataset after optimal microaggregation of key variables</caption>
<colgroup>
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 26%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><strong><em>Surface (m2)</em></strong></th>
<th style="text-align: right;"><strong><em>No.&nbsp;employees</em></strong></th>
<th style="text-align: right;"><strong><em>Turnover (Euros)</em></strong></th>
<th style="text-align: right;"><strong><em>Net profit (Euros)</em></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">3,212,334</td>
<td style="text-align: right;">313,250</td>
</tr>
<tr class="even">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">2,283,340</td>
<td style="text-align: right;">299,876</td>
</tr>
<tr class="odd">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">1,989,233</td>
<td style="text-align: right;">200,213</td>
</tr>
<tr class="even">
<td style="text-align: right;">756.67</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">984,983</td>
<td style="text-align: right;">143,211</td>
</tr>
<tr class="odd">
<td style="text-align: right;">756.67</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">194,232</td>
<td style="text-align: right;">51,233</td>
</tr>
<tr class="even">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">119,332</td>
<td style="text-align: right;">20,333</td>
</tr>
<tr class="odd">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">3,012,444</td>
<td style="text-align: right;">501,233</td>
</tr>
<tr class="even">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">4,233,312</td>
<td style="text-align: right;">777,882</td>
</tr>
<tr class="odd">
<td style="text-align: right;">756.67</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">159,999</td>
<td style="text-align: right;">60,388</td>
</tr>
<tr class="even">
<td style="text-align: right;">747.5</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">5,333,442</td>
<td style="text-align: right;">1,001,233</td>
</tr>
<tr class="odd">
<td style="text-align: right;">322.5</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">645,223</td>
<td style="text-align: right;">333,010</td>
</tr>
</tbody>
</table>
</div>
<section id="fixed-vs.-variable-group-size" class="level4">
<h4 class="anchored" data-anchor-id="fixed-vs.-variable-group-size">Fixed vs.&nbsp;variable group size</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Classical microaggregation algorithms (Defays and Nanopoulos, 1993) required that all groups except perhaps one be of size <span class="math inline">\(k\)</span>; allowing groups to be of size <span class="math inline">\(k\)</span> depending on the structure of data was termed <em>data-oriented microaggregation</em> (Mateo-Sanz and Domingo-Ferrer, 1999), (Domingo-Ferrer and Mateo-Sanz, 2002). <a href="#fig-fixed-vs-variable-sized-group">Figure&nbsp;1</a> illustrates the advantages of variable-sized groups. If classical fixed-size microaggregation with <span class="math inline">\(k = 3\)</span> is used, we obtain a partition of the data into three groups, which looks rather unnatural for the data distribution given. On the other hand, if variable-sized groups are allowed then the five data on the left can be kept in a single group and the four data on the right in another group; such a variable-size grouping yields more homogeneous groups, which implies lower information loss.</p>
<p>However, except for specific cases such as the one depicted in <a href="#fig-fixed-vs-variable-sized-group">Figure&nbsp;1</a>, the small gain in within-group homogeneity obtained with variable-sized groups hardly justifies the higher computational overhead of this option with respect to fixed-sized groups. This is particularly evident for multivariate data, as noted by&nbsp;Sande (2002).</p>
<div id="fig-fixed-vs-variable-sized-group" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Images/media/image66.png" style="width:3.8in;height:2.4in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Variable-sized groups versus fixed-sized groups</figcaption>
</figure>
</div>
</blockquote>
</section>
<section id="exact-optimal-vs.-heuristic-microaggregation" class="level4">
<h4 class="anchored" data-anchor-id="exact-optimal-vs.-heuristic-microaggregation">Exact optimal vs.&nbsp;heuristic microaggregation</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>For <span class="math inline">\(p = 1\)</span>, <em>i.e.</em> a univariate dataset or a multivariate dataset where variables are microaggregated one at a time, an exact polynomial shortest-path algorithm exists to find the <span class="math inline">\(k\)</span>-partition that optimally solves the microaggregation problem&nbsp;(Hansen and Mukherjee, 2003). See its description below.</p>
<p>For <span class="math inline">\(p &gt; 1\)</span>, finding an exact optimal solution to the microaggregation problem, <em>i.e.</em> finding a grouping where groups have maximal homogeneity and size at least <span class="math inline">\(k\)</span>, has been shown to be NP-hard&nbsp;(Oganian and Domingo-Ferrer, 2001).</p>
<p>Unfortunately, the univariate optimal algorithm by&nbsp;Hansen and Mukherjee (2003) is not very useful in practice and this for two reasons: i) microdata sets are normally multivariate and using univariate microaggregation to microaggregate them one variable at a time is not good in terms of disclosure risk (see&nbsp;Domingo-Ferrer et al., 2002); ii) although polynomial-time, the optimal algorithm is quite slow when the number of records is large.</p>
<p>Thus, practical methods in the literature are heuristic:</p>
<ul>
<li>Univariate methods deal with multivariate datasets by microaggregating one variable at a time, <em>i.e.</em> variables are sequentially and independently microaggregated. These heuristics are known as individual ranking&nbsp;(Defays and Nanopoulos, 1993). While they are fast and cause little information loss, these univariate heuristics have the same problem of high disclosure risk as univariate optimal microaggregation.</li>
<li>Multivariate methods either rank multivariate data by projecting them onto a single axis (<em>e.g.</em> using the first principal component or the sum of <span class="math inline">\(z\)</span>-scores (Defays and Nanopoulos, 1993) or directly deal with unprojected data&nbsp;(Mateo-Sanz and Domingo-Ferrer, 1999), (Domingo-Ferrer and Mateo-Sanz, 2002). When working on unprojected data, we can microaggregate all variables of the dataset at a time, or independently microaggregate groups of two variables at a time, three variables at a time, etc. In any case, it is preferable that variables within a group which is microaggregated at a time be correlated&nbsp;(W.E. Winkler, 2004) in order to keep as much as possible the analytic properties of the file.</li>
</ul>
<br>
We next describe the two microaggregation algorithms implemented in μ‑ARGUS.
</blockquote>
</section>
<section id="hansen-mukherjees-optimal-univariate-microaggregation" class="level4">
<h4 class="anchored" data-anchor-id="hansen-mukherjees-optimal-univariate-microaggregation">Hansen-Mukherjee’s optimal univariate microaggregation</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>In Hansen and Mukherjee (2003) a polynomial-time algorithm was proposed for <em>univariate</em> optimal microaggregation. Authors formulate the microaggregation problem as a shortest-path problem on a graph. They first construct the graph and then show that the optimal microaggregation corresponds to the shortest path in this graph. Each arc of the graph corresponds to a possible group that may be part of an optimal partition. The arc label is the <span class="math inline">\(\text{SSE}\)</span> that would result if that group were to be included in the partition. We next detail the graph construction.</p>
<p>Let <span class="math inline">\(V = \left\{ v_{1},\cdots,v_{n} \right\}\)</span> be a vector consisting of <span class="math inline">\(n\)</span> real numbers sorted into ascending order, so that <span class="math inline">\(v_{1}\)</span> is the smallest value and <span class="math inline">\(v_{n}\)</span> the largest value. Let <span class="math inline">\(k\)</span> be an integer group size such that <span class="math inline">\(1 \leq k &lt; n\)</span>. Now, a graph <span class="math inline">\(G_{n,k}\)</span> is constructed as follows:</p>
<ol type="1">
<li>For each value <span class="math inline">\(v_{i}\)</span> in <span class="math inline">\(V\)</span>, create a node with label <span class="math inline">\(i\)</span>. Create also an additional node with label 0.</li>
<li>For each pair of graph nodes <span class="math inline">\((i,j)\)</span> such that <span class="math inline">\(1 + k \leq j &lt; i + 2k\)</span>, create a directed arc <span class="math inline">\((i,j)\)</span>from node <span class="math inline">\(i\)</span> to node <span class="math inline">\(j\)</span>.</li>
<li>Map each arc <span class="math inline">\((i,j)\)</span> to the group of values <span class="math inline">\(C(i,j) = \left\{ v_{h}:i &lt; h \leq j \right\}\)</span>. Let the length <span class="math inline">\(L(i,j)\)</span> of the arc be the within group sum of squares for <span class="math inline">\(C(i,j)\)</span>, that is, <span class="math display">\[
L(i,j) = \sum\limits_{h = i + 1}^{j}\left( v_{h} - {\overline{v}}_{(i,j)} \right)^{2}
\]</span> where <span class="math inline">\({\overline{v}}_{(i,j)} = \frac{1}{j - i}\sum_{h=i+1}^{j}v_{h}\)</span></li>
</ol>
It is proven in&nbsp;Hansen and Mukherjee (2003) that the optimal <span class="math inline">\(k\)</span>-partition for <span class="math inline">\(V\)</span> is found by taking as groups the <span class="math inline">\(C(i,j)\)</span> corresponding to the arcs in the shortest path between nodes 0 and <span class="math inline">\(n\)</span>. For minimal group size <span class="math inline">\(k\)</span>and a dataset of <span class="math inline">\(n\)</span> real numbers sorted in ascending order, the complexity of this optimal univariate microaggregation is <span class="math inline">\(O\left( k^{2}n \right)\)</span>, that is, linear in the size of the dataset.
</blockquote>
</section>
<section id="the-mdav-heuristic-for-multivariate-microaggregation" class="level4">
<h4 class="anchored" data-anchor-id="the-mdav-heuristic-for-multivariate-microaggregation">The MDAV heuristic for multivariate microaggregation</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>The multivariate microaggregation heuristic implemented in μ‑ARGUS is called MDAV (Maximum Distance to Average Vector). MDAV performs multivariate fixed group size microaggregation on unprojected data. MDAV is also described in&nbsp;Domingo-Ferrer and Torra (2005).</p>
<p>MDAV Algorithm</p>
<ol type="1">
<li>Compute the average record <span class="math inline">\(\overline{x}\)</span> of all records in the dataset. Consider the most distant record <span class="math inline">\(x_{r}\)</span> to the average record <span class="math inline">\(\overline{x}\)</span> (using the squared Euclidean distance).</li>
<li>Find the most distant record <span class="math inline">\(x_{s}\)</span> from the record <span class="math inline">\(x_{r}\)</span> considered in the previous step.</li>
<li>Form two groups around <span class="math inline">\(x_{r}\)</span> and <span class="math inline">\(x_{s}\)</span>, respectively. One group contains <span class="math inline">\(x_{r}\)</span> and the <span class="math inline">\(k - 1\)</span> records closest to <span class="math inline">\(x_{r}\)</span>. The other group contains <span class="math inline">\(x_{s}\)</span> and the <span class="math inline">\(k - 1\)</span>records closest to <span class="math inline">\(x_{s}\)</span>.</li>
<li>If there are at least 3k records which do not belong to any of the two groups formed in Step&nbsp;3, go to Step&nbsp;1 taking as new dataset the previous dataset minus the groups formed in the last instance of Step&nbsp;3.</li>
<li>If there are between <span class="math inline">\(3k - 1\)</span> and <span class="math inline">\(2k\)</span> records which do not belong to any of the two groups formed in Step&nbsp;3:
<ol type="a">
<li>compute the average record <span class="math inline">\(\overline{x}\)</span> of the remaining records;</li>
<li>find the most distant record <span class="math inline">\(x_{r}\)</span> from <span class="math inline">\(\overline{x}\)</span>;</li>
<li>form a group containing <span class="math inline">\(x_{r}\)</span> and the <span class="math inline">\(k-1\)</span> records closest to <span class="math inline">\(x_{r}\)</span>;</li>
<li>form another group containing the rest of records. Exit the Algorithm.</li>
</ol></li>
<li>If there are less than <span class="math inline">\(2k\)</span> records which do not belong to the groups formed in Step&nbsp;3, form a new group with those records and exit the Algorithm.</li>
</ol>
<br>
The above algorithm can be applied independently to each group of variables resulting from partitioning the set of variables in the dataset.
</blockquote>
</section>
<section id="categorical-microaggregation" class="level4">
<h4 class="anchored" data-anchor-id="categorical-microaggregation">Categorical microaggregation</h4>
<blockquote class="blockquote">
<p>Recently&nbsp;(Torra, 2004), microaggregation has been extended to categorical data. Such an extension is based on existing definitions for aggregation and clustering, the two basic operations required in microaggregation. Specifically, the median is used for aggregating ordinal data and the plurality rule (voting) for aggregating nominal data. Clustering of categorical data is based on the <span class="math inline">\(k\)</span>-modes algorithm, which is a partitive clustering method similar to <span class="math inline">\(c\)</span>-means.</p>
</blockquote>
</section>
<section id="references-2" class="level4">
<h4 class="anchored" data-anchor-id="references-2">References</h4>
<p>Defays, D., and Nanopoulos, P. (1993<em>). Panels of enterprises and confidentiality: the small aggregates method</em>. In Proc. of 92 Symposium on Design and Analysis of Longitudinal Surveys, pages 195–204, Ottawa, 1993. Statistics Canada.</p>
<p>Domingo-Ferrer, J., and Mateo-Sanz, J. M. (2002). <em>Practical data-oriented microaggregation for statistical disclosure control</em>. IEEE Transactions on Knowledge and Data Engineering, 14(1):189–201, 2002.</p>
<p>Domingo-Ferrer, J., Mateo-Sanz, J. M., Oganian, A., and Torres, À. (2002). <em>On the security of microaggregation with individual ranking: analytical attacks</em>. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):477–492, 2002.</p>
<p>Domingo-Ferrer, J., and Torra, V. (2005). <em>Ordinal, continuous and heterogenerous k-anonymity through microaggregation.</em> Data Mining and Knowledge Discovery, 11(2):195–212, 2005.</p>
<p>Hansen, S. L. and Mukherjee, S. (2003<em>). A polynomial algorithm for optimal univariate microaggregation</em>. IEEE Transactions on Knowledge and Data Engineering, 15(4):1043–1044, 2003.</p>
<p>Hundepool, A., Van de Wetering, A., Ramaswamy, R., Franconi, L., Capobianchi, A., DeWolf, P.-P., Domingo-Ferrer, J., Torra, V., and Giessing, S. (2005<em>). μ-ARGUS version 4.0 Software and User’s Manual</em>. Statistics Netherlands, Voorburg NL, May 2005. <a href="https://research.cbs.nl/casc/deliv/MUmanual4.0.pdf"><u>https://research.cbs.nl/casc</u></a>.</p>
<p>Mateo-Sanz, J. M. and Domingo-Ferrer, J. (1999). <em>A method for data-oriented multivariate microaggregation</em>. In J.&nbsp;Domingo-Ferrer, editor, Statistical Data Protection, pages 89–99, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Oganian, A:, and Domingo-Ferrer, J. (2001). <em>On the complexity of optimal microaggregation for statistical disclosure control.</em> Statistical Journal of the United Nations Economic Comission for Europe, 18(4):345–354, 2001.</p>
<p>Samarati, P. (2001). <em>Protecting respondents’ identities in microdata release.</em> IEEE Transactions on Knowledge and Data Engineering, 13(6):1010–1027, 2001.</p>
<p>Samarati, P., and Sweeney, L. (1998). <em>Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression</em>. Technical report, SRI International, 1998.</p>
<p>Sande, G. (2002). <em>Exact and approximate methods for data directed microaggregation in one or more dimensions</em>. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(5):459–476, 2002.</p>
<p>Sweeney, L. (2002<em>). k-anonimity: A model for protecting privacy.</em> International Journal of Uncertainty, Fuzziness and Knowledge Based Systems, 10(5):557–570, 2002.</p>
<p>Torra, V. (2004). <em>Microaggregation for categorical variables: a median based approach</em>. In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of Lecture Notes in Computer Science, pages 162–174, Berlin Heidelberg, 2004. Springer.</p>
<p>Winkler, W. E. (2004). <em>Masking and re-identification methods for public-use microdata: overview and research problems.</em> In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of Lecture Notes in Computer Science, pages 231–246, Berlin Heidelberg, 2004. Springer.</p>
</section>
</section>
<section id="pram-1" class="level3">
<h3 class="anchored" data-anchor-id="pram-1">PRAM</h3>
<p>PRAM is a disclosure control technique that can be applied to categorical data. Basically, it is a form of intended misclassification, using a known and predetermined probability mechanism. Applying PRAM means that for each record in a microdata file, the score on one or more categorical variables is changed with a certain probability. This is done independently for each of the records. PRAM is thus a perturbative method. Since PRAM uses a probability mechanism, the disclosure risk is directly influenced by this method. An intruder can never be certain that a record she thinks she has identified is indeed the identified person: with a certain probability this has been a perturbed record.</p>
<p>Since the probability mechanism that is used when applying PRAM is known, characteristics of the (latent) true data can still be estimated from the perturbed data file. To that end, one can make use of correction methods similar to those used in case of misclassification and randomised response situations.</p>
<p>PRAM was used in 2001 UK Census to produce an end-user licence version of the Samples of Anonymised Records (SARs). See Gross(2004) <a href="http://www.ccsr.ac.uk/sars/events/2004-09-30/gross.pdf"><u>www.ccsr.ac.uk/sars/events/2004-09-30/gross.pdf</u></a> for a full description.</p>
<section id="pram-the-method" class="level4">
<h4 class="anchored" data-anchor-id="pram-the-method">PRAM, the method</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>In this section a short theoretical description of PRAM is given. For a detailed description of the method, see e.g., Gouweleeuw et al.&nbsp;(1998a and 1998b). For a discussion of several issues concerning the method and its consequences, see e.g., De Wolf et al.&nbsp;(1998).</p>
<p>Let <span class="math inline">\(\xi\)</span> denote a categorical variable in the original file to which PRAM will be applied and let <span class="math inline">\(X\)</span> denote the same variable in the perturbed file. Moreover, assume that <span class="math inline">\(\xi\)</span>, and hence <span class="math inline">\(X\)</span> as well, has <span class="math inline">\(K\)</span> categories, labelled <span class="math inline">\(1,\ldots,K\)</span>. The probabilities that define PRAM are denoted as</p>
<p><span class="math display">\[
p_{\text{kl}} = P(X = l \mid \xi = k)
\]</span></p>
<p><em>i.e.</em>, the probability that an original score <span class="math inline">\(\xi = k\)</span> is changed into the score <span class="math inline">\(X = l\)</span>. These so called transition probabilities are defined for all <span class="math inline">\(k, l = 1, ..., K\)</span>.<br>
Using these transition probabilities as entries of a <span class="math inline">\(K \times K\)</span> matrix, we obtain a Markov matrix that we will call the PRAM-matrix, denoted by <span class="math inline">\(\mathbf{P}\)</span>.</p>
<p>Applying PRAM now means that, given the score <span class="math inline">\(\xi = k\)</span> for record <span class="math inline">\(r\)</span>, the score <span class="math inline">\(X\)</span> for that record is drawn from the probability distribution <span class="math inline">\(p_{k1},\ldots,p_{kK}\)</span>. For each record in the original file, this procedure is performed independently of the other records.</p>
<p>To illustrate the ideas, suppose that the variable <span class="math inline">\(\xi\)</span> is gender, with scores <span class="math inline">\(\xi =1\)</span> if male and <span class="math inline">\(\xi = 2\)</span> if female. Applying PRAM with <span class="math inline">\(p_{11} = p_{22} = 0.9\)</span> on a microdata file with 110 males and 90 females, would yield a perturbed microdata file with <em>in expectation</em>, 108 males and 92 females. However, in expectation, 9 of these males were originally female, and similarly, 11 of the females were originally male.</p>
<p><em>Correcting analyses</em><br>
More generally, the effect of PRAM on one-dimensional frequency tables is that</p>
<p><span class="math display">\[
\mathbb{E}(T_{X} \mid \xi) = \mathbf{P}^t T_{\xi}
\]</span></p>
<p>where <span class="math inline">\(T_{\xi} = (T_{\xi}(1),\ldots,T_{\xi}(K))^T\)</span> denotes the frequency table according to the original microdata file and <span class="math inline">\(T_X\)</span> the frequency table according to the perturbed microdata file. A conditionally unbiased estimator of the frequency table in the original file is then given by</p>
<p><span class="math display">\[
{\hat{T}}_{\xi} = \left( \mathbf{P}^{- 1} \right)^t T_{X}
\]</span></p>
<p>This can be extended to two-dimensional frequency tables, by vectorizing such tables. The corresponding PRAM-matrix is then given by the Kronecker product of the PRAM-matrices of the individual dimensions.</p>
<p>Alternatively, one could use the two-dimensional frequency tables<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math inline">\(T_{\xi\eta}\)</span> for the original data and <span class="math inline">\(T_{XY}\)</span> for the perturbed data directly in matrix notation:</p>
<p><span class="math display">\[
\hat{T}_{\xi\eta} = \left( \mathbf{P}_{X}^{- 1} \right)^t T_{XY}\mathbf{P}_{Y}^{- 1}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{P}_{X}\)</span> denotes the PRAM-matrix corresponding to the categorical variable <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathbf{P}_{Y}\)</span> denotes the PRAM-matrix corresponding to the categorical variable <span class="math inline">\(Y\)</span>.</p>
<p>For more information about correction methods for statistical analyses applied to data that have been protected with PRAM, we refer to e.g., Gouweleeuw et al.&nbsp;(1998a) and Van den Hout (1999 and 2004).</p>
<p><em>Choice of PRAM-matrix</em><br>
The exact choice of transition probabilities influences both the amount of information loss as well as the amount of disclosure limitation. Moreover, in certain situations ‘illogical’ changes could occur, e.g., changing the gender of a female respondent with ovarian cancer to male. These kind of changes would attract the attention of a possible intruder which should be avoided.</p>
<p>It is thus important to choose the transition probabilities in an appropriate way. Illogical changes could be avoided by appointing a probability of 0 to the illogical scores. In the example given above, PRAM should not be applied to the variable gender individually, but to the crossing of the variables gender and diseases. In that case, each transition probability of changing a score into the score (male, ovarian cancer) should be set equal to 0.</p>
The choice of the transition probabilities in relation to the disclosure limitation and the information loss is more delicate. An empirical study on these effects is given in De Wolf and Van Gelder (2004). A theoretical discussion on the possibility to choose the transition probabilities in an optimal way (in some sense) is given in Cator et al.&nbsp;(2005).
</blockquote>
</section>
<section id="when-to-use-pram" class="level4">
<h4 class="anchored" data-anchor-id="when-to-use-pram">When to use PRAM</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>In certain situations methods like global recoding, local suppression and top-coding would yield too much loss of detail in order to produce a safe microdata file. In these circumstances, PRAM is an alternative. Using PRAM, the amount of detail is preserved whereas the level of disclosure control is achieved by introducing uncertainty in the scores on identifying variables.</p>
<p>However, in order to make adequate inferences on a microdata file to which PRAM has been applied, the statistician needs to include sophisticated changes to the standard methods. This demands a good knowledge of both PRAM and the statistical analysis that is to be applied.</p>
<p>In case a researcher is willing to make use of a remote execution facility, PRAM might be used to produce a microdata file with the same structure as the original microdata file, but with some kind of synthetic data. Such microdata files might be used as a ‘test’ microdata file on which a researcher can try her scripts before sending these scripts to the remote execution facility. Since the results of the script are not used directly, the amount of distortion of the original microdata file can be chosen to be quite large. That way a safe microdata file is produced that still exhibits the same structure (and amount of detail) as the original microdata file.</p>
In other situations, PRAM might produce a microdata file that is safe and leaves certain statistical characteristics of that file (more or less) unchanged. In that case, a researcher might perform his research on that microdata file in order to get an idea on the eventually needed research strategy. Once that strategy has been determined, the researcher might come to an on-site facility in order to perform the analyses once more on the original microdata hence reducing the amount of time that she has to be at the on-site facility.
</blockquote>
</section>
<section id="references-on-pram" class="level4">
<h4 class="anchored" data-anchor-id="references-on-pram">References on Pram</h4>
<p>Gross, B., Guiblin,Ph, and K. Merrett (2004), <em>Implementing the Post Randomisation method To the Individual Sample of Anonymised Records (SAR) from the 2001 Census</em> , Office for National Statistics. <a href="http://www.ccsr.ac.uk/sars/events/2004-09-30/gross.pdf"><u>www.ccsr.ac.uk/sars/events/2004-09-30/gross.pdf</u></a></p>
<p>Cator, E., Hensbergen A. and Y. Rozenholc (2005), <em>Statistical Disclosure Control using PRAM</em>, Proceedings of the 48th European Study Group Mathematics with Industry, Delft, The Netherlands, 15-19 March 2004. Delft University Press, 2005, p.&nbsp;23 – 30.</p>
<p>Gouweleeuw, J.M., P. Kooiman, L.C.R.J. Willenborg and P.P. de Wolf (1998a), <em>Post Randomisation for Statistical Disclosure Control: Theory and Implementation,</em> Journal of Official Statistics, Vol. 14, 4, pp.&nbsp;463 – 478.</p>
<p>Gouweleeuw, J.M., P. Kooiman, L.C.R.J. Willenborg and P.P. de Wolf (1998b), <em>The post randomisation method for protecting microdata</em>, Qüestiió, Quaderns d’Estadística i Investigació Operativa, Vol. 22, 1, pp.&nbsp;145 – 156.</p>
<p>Van den Hout, A. (2000), <em>The analysis of data perturbed by PRAM</em>, Delft University Press, ISBN 90-407-2014-2.</p>
<p>Van den Hout, A. (2004), <em>Analyzing misclassified data: randomized response and post randomization,</em> Ph.D.&nbsp;thesis, Utrecht University.</p>
<p>De Wolf, P.P. and I. Van Gelder (2004), <em>An empirical evaluation of PRAM,</em> Discussion paper 04012, Statistics Netherlands. This paper can also be found on the CASC-Website (<a href="https://research.cbs.nl/casc/Related/discussion-paper-04012.pdf"><u>https://research.cbs.nl/casc</u></a> Related Papers)</p>
<p>De Wolf, P.P., J.M. Gouweleeuw, P. Kooiman and L.C.R.J. Willenborg (1998), <em>Reflections on PRAM,</em> Proceedings of the conference “Statistical Data Protection”, March 25-27 1998, Lisbon, Portugal. This paper can also be found on the CASC-Website (<a href="https://research.cbs.nl/casc/related/Sdp_98_2.pdf"><u>https://research.cbs.nl/casc/related/Sdp_98_2.pdf</u></a>)</p>
</section>
</section>
<section id="synthetic-microdata" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-microdata">Synthetic microdata</h3>
<p>Publication of synthetic —<em>i.e.</em> simulated— data was proposed long ago as a way to guard against statistical disclosure. The idea is to randomly generate data with the constraint that certain statistics or internal relationships of the original dataset should be preserved.</p>
<p>We next review some approaches in the literature to synthetic data generation and then proceed to discuss the global pros and cons of using synthetic data.</p>
<section id="a-forerunner-data-distortion-by-probability-distribution" class="level4">
<h4 class="anchored" data-anchor-id="a-forerunner-data-distortion-by-probability-distribution">A forerunner: data distortion by probability distribution</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Data distortion by probability distribution was proposed in 1985&nbsp;(Liew, Choi and Liew, 1985) and is not usually included in the category of synthetic data generation methods. However, its operating principle is to obtain a protected dataset by randomly drawing from the underlying distribution of the original dataset. Thus, it can be regarded as a forerunner of synthetic methods.</p>
<p>This method is suitable for both categorical and continuous variables and consists of three steps:</p>
<ol type="1">
<li>Identify the density function underlying to each of the confidential variables in the dataset and estimate the parameters associated with that density function.</li>
<li>For each confidential variable, generate a protected series by randomly drawing from the estimated density function.</li>
<li>Map the confidential series to the protected series and publish the protected series instead of the confidential ones.</li>
</ol>
<p>In the identification and estimation stage, the original series of the confidential variable (<em>e.g.</em> salary) is screened to determine which of a set of predetermined density functions fits the data best. Goodness of fit can be tested by the Kolmogorov-Smirnov test. If several density functions are acceptable at a given significance level, selecting the one yielding the smallest value for the Kolmogorov-Smirnov statistics is recommended. If no density in the predetermined set fits the data, the frequency imposed distortion method can be used. With the latter method, the original series is divided into several intervals (somewhere between 8 and 20). The frequencies within the interval are counted for the original series, and become a guideline to generate the distorted series. By using a uniform random number generating subroutine, a distorted series is generated until its frequencies become the same as the frequencies of the original series. If the frequencies in some intervals overflow, they are simply discarded.</p>
<p>Once the best-fit density function has been selected, the generation stage feeds its estimated parameters to a random value generating routine to produce the distorted series.</p>
<p>Finally, the mapping and replacement stage is only needed if the distorted variables are to be used jointly with other non-distorted variables. Mapping consists of ranking the distorted series and the original series in the same order and replacing each element of the original series with the corresponding distorted element.</p>
<p>It must be stressed here that the approach described in&nbsp;(Liew, Choi and Liew, 1985) was for one variable at a time. One could imagine a generalization of the method using multivariate density functions. However such a generalization: i) is not trivial, because it requires multivariate ranking-mapping; and ii) can lead to very poor fitting.</p>
<strong>Example 1</strong> A distribution fitting software&nbsp;(Crystal.Ball, 2004) has been used on the original (ranked) data set 186, 693, 830, 1177, 1219, 1428, 1902, 1903, 2496, 3406. Continuous distributions tried were normal, triangular, exponential, lognormal, Weibull, uniform, beta, gamma, logistic, Pareto and extreme value; discrete distributions tried were binomial, Poisson, geometric and hypergeometric. The software allowed for three fitting criteria to be used: Kolmogorov-Smirnov, <span class="math inline">\(\chi^{2}\)</span> and Anderson-Darling. According to the first criterion, the best fit happened for the extreme value distribution with modal and scale parameters 1105.78 and 732.43, respectively; the Kolmogorov statistic for this fit was 0.1138. Using the fitted distribution, the following (ranked) dataset was generated and used to replace the original one: 425.60, 660.97, 843.43, 855.76, 880.68, 895.73, 1086.25, 1102.57, 1485.37, 2035.34.
</blockquote>
</section>
<section id="synthetic-data-by-multiple-imputation" class="level4">
<h4 class="anchored" data-anchor-id="synthetic-data-by-multiple-imputation">Synthetic data by multiple imputation</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Rubin (1993) suggested creating an entirely synthetic dataset based on the original survey data and multiple imputations. Rubin’s proposal was more completely developed in Raghunathan, Reiter, and Rubin (2003). A simulation study of it was given in Reiter (2002). In Reiter (2005) inference on synthetic data is discussed and in Reiter (2005b) an application is given.</p>
We next sketch the operation of the original proposal by Rubin. Consider an original microdata set <span class="math inline">\(X\)</span> of size <span class="math inline">\(n\)</span> records drawn from a much larger population of <span class="math inline">\(N\)</span> individuals, where there are background variables <span class="math inline">\(A\)</span>, non-confidential variables <span class="math inline">\(B\)</span> and confidential variables <span class="math inline">\(C\)</span>. Background variables are observed and available for all <span class="math inline">\(N\)</span> individuals in the population, whereas <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are only available for the <span class="math inline">\(n\)</span> records in the sample <span class="math inline">\(X\)</span>. The first step is to construct from <span class="math inline">\(X\)</span> a multiply-imputed population of <span class="math inline">\(N\)</span> individuals. This population consists of the <span class="math inline">\(n\)</span> records in <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span> (the number of multiple imputations, typically between 3 and 10) matrices of <span class="math inline">\((B,C)\)</span> data for the <span class="math inline">\(N - n\)</span> non-sampled individuals. The variability in the imputed values ensures, theoretically, that valid inferences can be obtained on the multiply-imputed population. A model for predicting <span class="math inline">\((B,C)\)</span> from <span class="math inline">\(A\)</span> is used to multiply-impute <span class="math inline">\((B,C)\)</span> in the population. The choice of the model is a nontrivial matter. Once the multiply-imputed population is available, a sample <span class="math inline">\(Z\)</span> of <span class="math inline">\(n'\)</span> records can be drawn from it whose structure looks like the one a sample of <span class="math inline">\(n'\)</span> records drawn from the original population. This can be done <span class="math inline">\(M\)</span> times to create <span class="math inline">\(M\)</span> replicates of <span class="math inline">\((B,C)\)</span> values. The results are <span class="math inline">\(M\)</span> multiply-imputed synthetic datasets. To make sure no original data are in the synthetic datasets, it is wise to draw the samples from the multiply-imputed population excluding the <span class="math inline">\(n\)</span> original records from it.
</blockquote>
</section>
<section id="synthetic-data-by-bootstrap" class="level4">
<h4 class="anchored" data-anchor-id="synthetic-data-by-bootstrap">Synthetic data by bootstrap</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Fienberg (1994) proposed generating synthetic microdata by using bootstrap methods. Later, in Fienberg, Makov and Steele (1998), this approach was used for categorical data.</p>
The bootstrap approach bears some similarity to the data distortion by probability distribution and the multiple-imputation methods described above. Given an original microdata set <span class="math inline">\(X\)</span> with <span class="math inline">\(p\)</span> variables, the data protector computes its empirical <span class="math inline">\(p\)</span>-variate cumulative distribution function (c.d.f.) <span class="math inline">\(F\)</span>. Now, rather than distorting the original data to obtain masked data, the data protector alters (or “smoothes”) the c.d.f. <span class="math inline">\(F\)</span> to derive a similar c.d.f. <span class="math inline">\(F'\)</span>. Finally, <span class="math inline">\(F'\)</span> is sampled to obtain a synthetic microdata set <span class="math inline">\(Z\)</span>.
</blockquote>
</section>
<section id="synthetic-data-by-latin-hypercube-sampling" class="level4">
<h4 class="anchored" data-anchor-id="synthetic-data-by-latin-hypercube-sampling">Synthetic data by Latin Hypercube Sampling</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
Latin Hypercube Sampling (LHS) appears in the literature as another method for generating multivariate synthetic datasets. In Huntington and Lyrintzis (1998), the LHS updated technique of Florian (1992) was improved, but the proposed scheme is still time-intensive even for a moderate number of records. In Dandekar, Cohen and Kirkendall (2002) LHS is used along with a rank correlation refinement to reproduce both the univariate (<em>i.e.</em> mean and covariance) and multivariate structure (in the sense of rank correlation) of the original dataset. In a nutshell, LHS-based methods rely on iterative refinement, are time-intensive and their running time does not only depend on the number of values to be reproduced, but on the starting values as well.
</blockquote>
</section>
<section id="partially-synthetic-data-by-cholesky-decomposition" class="level4">
<h4 class="anchored" data-anchor-id="partially-synthetic-data-by-cholesky-decomposition">Partially synthetic data by Cholesky decomposition</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Generating plausible synthetic values for all variables in a database may be difficult in practice. Thus, several authors have considered mixing actual and synthetic data.</p>
<p>In Burridge (2004) a family of methods known as IPSO (Information Preserving Statistical Obfuscation) is proposed for generation of partially synthetic data. It consists of three methods that are described next.</p>
<p><em>Method A: The basic IPSO procedure</em><br>
The basic form of IPSO will be called here Method A. Informally, suppose two sets of variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where the former are the confidential outcome variables and the latter are quasi-identifier variables. Then <span class="math inline">\(X\)</span> are taken as independent and <span class="math inline">\(Y\)</span> as dependent variables. A multiple regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> is computed and fitted <span class="math inline">\(Y_{A}'\)</span> variables are computed. Finally, variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_{A}'\)</span> are released in place of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>More formally, let <span class="math inline">\(y\)</span>and <span class="math inline">\(x\)</span> be two data matrices, with rows representing respondents and columns representing variables; the row vectors <span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(x_{i}\)</span> will represent the data for the <span class="math inline">\(i\)</span>-th respondent, for <span class="math inline">\(i = 1,\cdots,n\)</span>. The column vector <span class="math inline">\(u_{j}\)</span> will represent the quasi-identifier variable <span class="math inline">\(j\)</span>, for <span class="math inline">\(j = 1,\cdots,p\)</span>; in other words, the <span class="math inline">\(u_{j}\)</span> are the columns of quasi-identifier matrix <span class="math inline">\(Y\)</span>. Conditionally on the specific values for confidential variables, quasi-identifier variables for different respondents are assumed to be independent. Conditional on the specific confidential variables <span class="math inline">\(x_{i}\)</span>, the quasi-identifier variables <span class="math inline">\(Y_{i}\)</span>. are assumed to follow a multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma = \left\{ \sigma_{jk} \right\}\)</span> and a mean vector <span class="math inline">\(x_{i}B\)</span>, where <span class="math inline">\(B\)</span> is an <span class="math inline">\(m\times p\)</span> matrix with columns <span class="math inline">\(\beta_{j}\)</span>. Thus a separate univariate normal multiple regression model is assumed for each column of <span class="math inline">\(Y\)</span> with regression parameter equal to the corresponding column of <span class="math inline">\(B\)</span>, that is, <span class="math inline">\(U_{j} \sim N\left( x\beta_{j},\sigma_{jj}I \right)\)</span>.</p>
<p>Let <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> be the maximum likelihood estimates of <span class="math inline">\(B\)</span> and <span class="math inline">\(\Sigma\)</span> derived from the complete dataset <span class="math inline">\((y,x)\)</span>. These estimates are a pair of sufficient statistics for the regression model. We denote in what follows the vectors of fitted values and residuals for <span class="math inline">\(u_{j}\)</span> as <span class="math inline">\({\hat{\mu}}_{j}\)</span> and <span class="math inline">\({\hat{r}}_{j}\)</span>, respectively. Thus, <span class="math inline">\(\hat{\mu}\)</span>, <span class="math inline">\(\hat{r}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> will denote the matrices <span class="math inline">\(x\hat{B}\)</span>, <span class="math inline">\(y - x\hat{B}\)</span> and <span class="math inline">\(n^{- 1}{\hat{r}}^t\hat{r}\)</span>, respectively.</p>
<p>The output of IPSO Method A is <span class="math inline">\(y'_{A} = x\hat{B}\)</span>.</p>
<p><em>Method B: IPSO preserving <span class="math inline">\(\hat{B}\)</span></em><br>
If a user fits a multiple regression model to <span class="math inline">\(\left( y_{A}',x \right)\)</span>, she will get estimates <span class="math inline">\({\hat{B}}_{A}\)</span> and <span class="math inline">\({\hat{\Sigma}}_{A}\)</span> which, in general, are different from the estimates <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> obtained when fitting the model to the original data <span class="math inline">\((y,x)\)</span>.</p>
<p>IPSO Method B modifies <span class="math inline">\(y_{A}'\)</span> into <span class="math inline">\(y_{B}'\)</span> in such a way that the estimate <span class="math inline">\({\hat{B}}_{B}\)</span> obtained by multiple linear regression from <span class="math inline">\(\left( y_{B}',x \right)\)</span> satisfies <span class="math inline">\({\hat{B}}_{B} = \hat{B}\)</span>.</p>
<p>Suppose that <span class="math inline">\(\tilde{y}\)</span> is a new, artificial, set of quasi-identifier values. These can be any set of numbers initially, <em>e.g.</em> an i.i.d. normal random sample or a deterministically chosen set. For each component new residuals <span class="math inline">\({\tilde{r}}_{j}\)</span> are calculated by fitting the above multivariate multiple regression to the new “data” <span class="math inline">\(\tilde{y}\)</span>. Define</p>
<p><span class="math display">\[
y_{B}' = \hat{\mu} + \tilde{r} = x\hat{B} + \tilde{r}
\]</span></p>
<p>The following information preservation result holds for IPSO-B.</p>
<p><strong>Lemma 3.3.7.1.</strong> <em>Regardless of the initial choice <span class="math inline">\(\tilde{y}\)</span>, <span class="math inline">\(\left( y_{B}',x \right)\)</span> preserves the sufficient statistic <span class="math inline">\(\hat{B}\)</span>.</em></p>
<p><strong>Proof:</strong> We have that <span class="math display">\[
y_{B}' = x\hat{B} + \tilde{r} = x\hat{B} + \left( \tilde{y} - x\tilde{B} \right) \qquad \text{(3.3.5.1)}
\]</span> where <span class="math inline">\(\hat{B}\)</span> is the MLE estimate of <span class="math inline">\(B\)</span> obtained from <span class="math inline">\(\left( \tilde{y},x \right)\)</span>. Now, the expressions of <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\tilde{B}\)</span> are, respectively, <span class="math display">\[
\hat{B} = \left( x^{t}x \right)^{- 1}x^{t}y
\]</span> and <span class="math display">\[
\tilde{B} = \left( x^{t}x \right)^{- 1}x^{t}\tilde{y}
\]</span> Analogously, the expression of the MLE estimate of <span class="math inline">\({\hat{B}}_{B}\)</span> obtained from <span class="math inline">\(\left( y_{B}',x \right)\)</span> is <span class="math display">\[
{\hat{B}}_{B} = \left( x^{t}x \right)^{- 1}x^{t}y_{B}'
\]</span> Substituting expression (3.3.5.1) for <span class="math inline">\(y_{B}'\)</span> in the equation above, we get <span class="math display">\[
{\hat{B}}_{B} = \left( x^{t}x \right)^{- 1}\left( x^{t}x \right)\hat{B} + \left( x^{t}x \right)^{- 1}x^{t}(\tilde{y} - x\tilde{B}) = \hat{B} + \tilde{B} - \tilde{B} = \hat{B}
\]</span><br>
<br>
<em>Method C: IPSO preserving <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span></em><br>
A more ambitious goal is to come up with a data matrix <span class="math inline">\(y_{C}'\)</span> such that, when a multivariate multiple regression model is fitted to <span class="math inline">\(\left( y_{C}',x \right)\)</span>, <em>both</em> sufficient statistics <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> obtained on the original data <span class="math inline">\((y,x)\)</span> are preserved.</p>
<p>The algorithm proposed in&nbsp;Burridge (2004) to get <span class="math inline">\(y_{C}'\)</span> is as follows</p>
<ol type="1">
<li>Generate provisional new “data” <span class="math inline">\(\tilde{y}\)</span> (this will be an <span class="math inline">\(n\times p\)</span> matrix).</li>
<li>Calculate provisional new residuals <span class="math inline">\(\tilde{r}\)</span> by fitting the multiple regression model to each column of <span class="math inline">\(\tilde{y}\)</span>.</li>
<li>Define new residuals <span class="math inline">\({\tilde{r}}'\)</span> as a transformation of <span class="math inline">\(\tilde{r}\)</span> so that <span class="math inline">\({\tilde{r}}^t{\tilde{r}}' = n\hat{\Sigma}\)</span>. This is easily done as follows:
<ol type="a">
<li>Let <span class="math inline">\(L\)</span> and <span class="math inline">\(L_{O}\)</span> be the lower triangular matrices in the Cholesky factorizations <span class="math inline">\(n\hat{\Sigma} = LL^{t}\)</span> and <span class="math inline">\({\tilde{r}}^t\tilde{r} = L_{O}^{\strut}L_{O}^t\)</span>.</li>
<li>Define <span class="math inline">\({\tilde{r}}' = \tilde{r}\left(L_{O}^{-1}\right)^t L^t\)</span>. It is easily verified that <span class="math inline">\(({\tilde{r}}')^t {\tilde{r}}' = n\hat{\Sigma}\)</span>.</li>
</ol></li>
</ol>
<p>Information preservation in IPSO-C is as follows.</p>
<p>Define <span class="math display">\[
y_{C}' = x\hat{B} + {\tilde{r}}'
\]</span></p>
<p><strong>Lemma 3.3.7.2.</strong> <em><span class="math inline">\(\left( y_{C}',x \right)\)</span> preserves the sufficient statistics <span class="math inline">\(\hat{B}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span>.</em></p>
<p><strong>Proof:</strong> The expression of the MLE estimate of <span class="math inline">\(\hat{B}\)</span> obtained from <span class="math inline">\(\left( y_{C}',x \right)\)</span> is <span class="math display">\[
\begin{align}
{\hat{B}}_{C} &amp;= \left( x^t x \right)^{- 1}x^t y_{C}' = \left( x^t x \right)^{- 1}x^t \left( x\hat{B} + {\tilde{r}}' \right) \\
&amp;= \hat{B} + \left( x^t x \right)^{- 1}x^t \tilde{r}L_{O}^t L^t = \hat{B} + \left( x^t x \right)^{- 1}x^t \left( \tilde{y} - x\tilde{B} \right)L_{O}^t L^t \\
&amp;= \hat{B} + \left( \tilde{B} - \tilde{B} \right)L_{O}^t L^t = \hat{B} \\
\end{align}
\]</span> Using that <span class="math inline">\({\hat{B}}_{C} = \hat{B}\)</span>, the expression of the MLE estimate of <span class="math inline">\(\hat{\Sigma}\)</span> obtained from <span class="math inline">\(\left( y_{C}',x \right)\)</span> is <span class="math display">\[
\begin{align}
{\hat{\Sigma}}_{C} &amp;= \frac{\left( y_{C}',x\hat{B} \right)^t \left( y_{C}',x\hat{B} \right)}{n}\\
&amp;= \frac{\left( x\hat{B} + {\tilde{r}}' - x\hat{B} \right)^t \left( x\hat{B} + {\tilde{r}}' - x\hat{B} \right)}{n} \\
&amp;= \frac{{\tilde{r}}^t {\tilde{r}}'}{n} \\
&amp;= \hat{\Sigma}
\end{align}
\]</span></p>
<p>where in the last equality we have used the property required on <span class="math inline">\({\tilde{r}}'\)</span>.</p>
<p><em>Using IPSO to get entirely synthetic microdata</em><br>
In&nbsp;Mateo-Sanz, Martínez-Ballesté and Domingo-Ferrer (2004), a non-iterative method for generating entirely synthetic continuous microdata through Cholesky decomposition is proposed. This can be viewed as a special case of IPSO. In a single step of computation, the method exactly reproduces the means and the covariance matrix of the original dataset. The running time grows linearly with the number of records. Exact preservation of the original covariance matrix implies that variances and Pearson correlations are also exactly preserved in the synthetic dataset.</p>
The idea of the method is as follows. A dataset <span class="math inline">\(X\)</span> is viewed as a <span class="math inline">\(n\times m\)</span> matrix, where rows are records and columns are variables. First, the covariance matrix <span class="math inline">\(C\)</span> of <span class="math inline">\(X\)</span> is computed (covariance is defined between variables, <em>i.e.</em> between columns). Then, a random <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(A\)</span> is generated, whose covariance matrix is the identity matrix. Next, the Cholesky decomposition of <span class="math inline">\(C\)</span> is computed, <em>i.e.</em>, an upper triangular matrix <span class="math inline">\(U\)</span> is found such that <span class="math inline">\(C=U^t U\)</span>. Finally, the synthetic microdata set <span class="math inline">\(Z\)</span> is an <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(Z = A U\)</span>.
</blockquote>
</section>
<section id="other-partially-synthetic-microdata-approaches" class="level4">
<h4 class="anchored" data-anchor-id="other-partially-synthetic-microdata-approaches">Other partially synthetic microdata approaches</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>The multiple imputation approach described in Rubin (1993) for creating entirely synthetic microdata can be extended for partially synthetic microdata. As a result, multiply-imputed, partially synthetic datasets are obtained that contain a mix of actual and imputed (synthetic) values. The idea is to multiply-impute confidential values and release non-confidential values without perturbation. This approach was first applied to protect the US Survey of Consumer Finances&nbsp;(Kennickell, 1999), (Kennickell, 1999b). In Abowd and Woodcock (2001) and Abowd and Woodcock (2004), this technique was adopted to protect longitudinal linked data, that is, microdata that contain observations from two or more related time periods (successive years, etc.). Methods for valid inference on this kind of partial synthetic data were developed in Reiter (2003) and a non-parametric method was presented in&nbsp;Reiter (2003b) to generate multiply-imputed, partially synthetic data.</p>
Closely related to multiply imputed, partially synthetic microdata is model-based disclosure protection&nbsp;(Franconi and Stander, 2002), (Polettini, Franconi, and Stander, 2002). In this approach, a set of confidential continuous outcome variables is regressed on a disjoint set non-confidential variables; then the fitted values are released for the confidential variables instead of the original values.
</blockquote>
</section>
<section id="muralidhar-sarathy-hybrid-generator" class="level4">
<h4 class="anchored" data-anchor-id="muralidhar-sarathy-hybrid-generator">Muralidhar-Sarathy hybrid generator</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Hybrid data are a mixture of original data and synthetic data. Let <span class="math inline">\(V\)</span> an original data set whose attributes are numerical and fall into confidential attributes <span class="math inline">\(X (=X_1\dots X_L)\)</span> and non-confidential attributes <span class="math inline">\(Y (=Y_1\dots Y_M)\)</span>. Let <span class="math inline">\(V'\)</span> be a hybrid data set obtained from <span class="math inline">\(V\)</span>, whose attributes are <span class="math inline">\(X (=X'_1\dots X'_L)\)</span> (hybrid versions of <span class="math inline">\(X\)</span>) and <span class="math inline">\(Y\)</span>.</p>
<p>Muralidhar and Sarathy (2008) proposed a procedure (called MS in the sequel) for generating hybrid data as follows <span class="math display">\[
X'_j = \gamma + X_j\alpha^t + Y_j\beta^t + e_i, \quad j = 1, \dots, n
\]</span> MS can yield hybrid data preserving the means and covariances of original data. To that end, the following equalities must be satisfied: <span class="math display">\[
\begin{align}
\beta^t &amp;= \Sigma_{YY}^{-1} \Sigma_{YX}^{\strut} (I-\alpha^t) \\
\gamma &amp;= (I-\alpha) \bar{X} - \beta \bar{Y} \\
\Sigma_{ee} &amp;= (\Sigma_{XX}^{\strut} - \Sigma_{XY}^{\strut}\Sigma_{YY}^{-1}\Sigma_{YX}^{\strut}) - \alpha (\Sigma_{XX}^{\strut} - \Sigma_{XY}^{\strut}\Sigma_{YY}^{-1}\Sigma_{YX}^{\strut}) \alpha^t
\end{align}
\]</span> where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(\Sigma_{ee}\)</span> is the covariance matrix of the noise terms <span class="math inline">\(e\)</span>.</p>
<p>Thus, <span class="math inline">\(\alpha\)</span> completely specifies the procedure. The authors of MS admit that <span class="math inline">\(\alpha\)</span> must be selected carefully to ensure that <span class="math inline">\(\Sigma_{ee}\)</span> is positive semidefinite. They consider three options for specifying the <span class="math inline">\(\alpha\)</span> matrix:</p>
<ol type="1">
<li>Take <span class="math inline">\(\alpha\)</span> as a diagonal matrix with all values in the diagonal being equal. In this case, <span class="math inline">\(\Sigma_{ee}\)</span> is positive semidefinite and the value of the hybrid attribute <span class="math inline">\(X_i'\)</span> depends only on <span class="math inline">\(X_i\)</span>, but not on <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j \neq i\)</span>. All confidential attributes <span class="math inline">\(X_i\)</span> are perturbed at the same level.</li>
<li>Take <span class="math inline">\(\alpha\)</span> as a diagonal matrix, with values in the diagonal being not all equal. In this case, <span class="math inline">\(X_i'\)</span> still depends only on <span class="math inline">\(X_i\)</span>, but not on <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j \neq i\)</span>. The differences are that the confidential attributes are perturbed at different levels and there is no guarantee that <span class="math inline">\(\Sigma_{ee}\)</span> is positive semidefinite, so it may be necessary to try several values of <span class="math inline">\(\alpha\)</span> until positive semidefiniteness is achieved.</li>
<li>Taking <span class="math inline">\(\alpha\)</span> as a non-diagonal matrix does not guarantee positive semidefiniteness either and the authors of MS do not see any advantage in it, although it would be the only way to have <span class="math inline">\(X_i'\)</span> depend on several attributes among <span class="math inline">\((X_1 \dots X_L)\)</span>. With <span class="math inline">\(\mathbb{R}\)</span>-Microhybrid, the dependence of <span class="math inline">\(X_i'\)</span> on the original confidential attributes is the one provided by the underlying IPSO method.</li>
</ol>
</blockquote></section>
<section id="microaggregation-based-hybrid-data" class="level4">
<h4 class="anchored" data-anchor-id="microaggregation-based-hybrid-data">Microaggregation-based hybrid data</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>In (Domingo-Ferrer and González-Nicolás, 2009) an alternative procedure to generate hybrid data based on microaggregation was proposed. Let <span class="math inline">\(V\)</span> be an original data set consisting of <span class="math inline">\(n\)</span> records. On input an integer parameter <span class="math inline">\(k \in \{1,\dots,n\}\)</span>, the procedure described in this section generates a hybrid data set <span class="math inline">\(V'\)</span>. The greater <span class="math inline">\(k\)</span>, the more synthetic is <span class="math inline">\(V'\)</span>. Extreme cases are: i) <span class="math inline">\(k\)</span> = 1, which yields <span class="math inline">\(V' = V\)</span> (the output data are exactly the original input data); and ii) <span class="math inline">\(k = n\)</span>, which yields a completely synthetic output data set <span class="math inline">\(V'\)</span>.</p>
<p>The procedure calls two algorithms:</p>
<ul>
<li>A generic synthetic data generator <span class="math inline">\(S(C,C', \text{parms})\)</span>, that is, an algorithm which, given an original data (sub)set <span class="math inline">\(C\)</span>, generates a synthetic data (sub)set <span class="math inline">\(C'\)</span> preserving the statistics or parameters or models of <span class="math inline">\(C\)</span> specified in <span class="math inline">\(\text{parms}\)</span>.</li>
<li>A microaggregation heuristic, which, on input of a set of <span class="math inline">\(n\)</span> records and parameter <span class="math inline">\(k\)</span>, partitions the set of records into clusters containing between <span class="math inline">\(k\)</span> and <span class="math inline">\(2k − 1\)</span> records. Cluster creation attempts to maximize intra-cluster homogeneity.</li>
</ul>
<p><strong>Procedure 1 (<em>Microhybrid</em> (<span class="math inline">\(V\)</span>,<span class="math inline">\(V'\)</span>, <span class="math inline">\(\text{parms}\)</span>, <span class="math inline">\(k\)</span>))</strong></p>
<ol type="1">
<li>Call microaggregation(<span class="math inline">\(V\)</span>, <span class="math inline">\(k\)</span>). Let <span class="math inline">\(C_1,\dots,C_k\)</span> for some <span class="math inline">\(k\)</span> be the resulting clusters of records.</li>
<li>For <span class="math inline">\(i = 1, \dots, k\)</span> call <span class="math inline">\(S(C_i,C_{i}', \text{parms})\)</span>.</li>
<li>Output a hybrid dataset <span class="math inline">\(V'\)</span> whose records are those in the clusters <span class="math inline">\(C_{1}',\dots,C_{k}'\)</span> .</li>
</ol>
<p>At Step 1 of procedure Microhybrid above, clusters containing between <span class="math inline">\(k\)</span> and <span class="math inline">\(2k −1\)</span> records are created. Then at Step 2, a synthetic version of each cluster is generated. At Step 3, the original records in each cluster are replaced by the records in the corresponding synthetic cluster (instead of replacing them with the average record of the cluster, as done in conventional microaggregation).</p>
<p>The Microhybrid procedure bears some resemblance to the condensation approach proposed by (Aggarwal and Yu, 2004); however, Microhybrid is more general because:</p>
<ul>
<li>It can be applied to any data type (condensation is designed for numerical data only);</li>
<li>Clusters do not need to be all of size <span class="math inline">\(k\)</span> (their sizes can vary between <span class="math inline">\(k\)</span> and <span class="math inline">\(2k − 1\)</span>);</li>
<li>Any synthetic data generator (chosen to preserve certain pre-selected statistics or models) can be used by Microhybrid;</li>
<li>Instead of using an ad hoc clustering heuristic like condensation, Microhybrid can use any of the best microaggregation heuristics cited above, which should yield higher within-cluster homogeneity and thus less information loss.</li>
</ul>
<p><em>Role of parameter <span class="math inline">\(k\)</span></em><br>
We justify here the role of parameter <span class="math inline">\(k\)</span> in Microhybrid:</p>
<ul>
<li>If <span class="math inline">\(k = 1\)</span>, and <span class="math inline">\(\text{parms}\)</span> include preserving the mean of each attribute in the original clusters, the output is the same original data set, because the procedure creates <span class="math inline">\(n\)</span> clusters (as many as the number of original records). With <span class="math inline">\(k = 1\)</span>, even variable-size heuristics will yield all clusters of size 1, because the maximum intra-cluster similarity is obtained when clusters consist all of a single record.</li>
<li>If <span class="math inline">\(k = n\)</span>, the output is a single synthetic cluster: the procedure is equivalent to calling the synthetic data generator <span class="math inline">\(S\)</span> once for the entire data set.</li>
<li>For intermediate values of <span class="math inline">\(k\)</span>, several clusters are obtained at Step 1, whose parameters <span class="math inline">\(\text{parms}\)</span> are preserved by the synthetic clusters generated at Step 2. As <span class="math inline">\(k\)</span> decreases, the number of clusters (whose parameters are preserved in the data output at Step 3) increases, which causes the output data to look more and more like the original data. Each cluster can be regarded as a constraint on the synthetic data generation: the more constraints, the less freedom there is for generating synthetic data, and the output resembles more the original data. This is why the output data can be called hybrid.</li>
</ul>
<p>It must be noted here that, depending on the synthetic generator used, there may be a lower bound for <span class="math inline">\(k\)</span> higher than 1. For example, if using IPSO (see section 3.4.7.5) with <span class="math inline">\(|X|\)</span> confidential attributes and <span class="math inline">\(|Y|\)</span> non-confidential attributes, it turns out that <span class="math inline">\(k\)</span> must be at least <span class="math inline">\(2|X|+|Y|+1\)</span>; otherwise there are not enough degrees of freedom for the generator to work.</p>
Note that the choice of parameter <span class="math inline">\(k\)</span> is more straightforward than the choice of <span class="math inline">\(\alpha\)</span> in the MS procedure above. Also, for the case of numerical microdata, Microhybrid can offer, in addition to mean and covariance exact preservation, approximate preservation of third-order and fourth-order moments, and also approximate preservation of all moments up to order four in randomly chosen subdomains of the dataset. Details are given in the above-referenced paper describing Microhybrid.
</blockquote>
</section>
<section id="other-hybrid-microdata-approaches" class="level4">
<h4 class="anchored" data-anchor-id="other-hybrid-microdata-approaches">Other hybrid microdata approaches</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>A different approach called hybrid masking was proposed in&nbsp;Dandekar, Domingo-Ferrer and Sebé (2002). The idea is to compute masked data as a combination of original and synthetic data. Such a combination allows better control than purely synthetic data over the individual characteristics of masked records. For hybrid masking to be feasible, a rule must be used to pair one original data record with one synthetic data record. An option suggested in&nbsp;Dandekar, Domingo-Ferrer and Sebé (2002) is to go through all original data records and pair each original record with the nearest synthetic record according to some distance. Once records have been paired,&nbsp;Dandekar, Domingo-Ferrer, and Sebé (2002) suggest two possible ways for combining one original record <span class="math inline">\(X\)</span> with one synthetic record <span class="math inline">\(X_{S}\)</span>: additive combination and multiplicative combination. Additive combination yields</p>
<p><span class="math display">\[
Z = \alpha X + (1 - \alpha)X_{S}
\]</span></p>
<p>and multiplicative combination yields</p>
<p><span class="math display">\[
Z = X^{\alpha} \cdot X_{s}^{(1 - \alpha)}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is an input parameter in <span class="math inline">\([0,1]\)</span> and <span class="math inline">\(Z\)</span> is the hybrid record. The authors present empirical results comparing the hybrid approach with rank swapping and microaggregation masking (the synthetic component of hybrid data is generated using Latin Hypercube Sampling&nbsp;by Dandekar, Cohen, and Kirkendall, 2002).</p>
Post-masking optimization is another approach to combining original and synthetic microdata is proposed in Sebé et al.&nbsp;(2002) . The idea here is to first mask an original dataset using a masking method. Then a hill-climbing optimization heuristic is run which seeks to modify the masked data to preserve the first and second-order moments of the original dataset as much as possible without increasing the disclosure risk with respect to the initial masked data. The optimization heuristic can be modified to preserve higher-order moments, but this significantly increases computation. Also, the optimization heuristic can use take as initial dataset a random dataset instead of a masked dataset; in this case, the output dataset is purely synthetic.
</blockquote>
</section>
<section id="pros-and-cons-of-synthetic-and-hybrid-microdata" class="level4">
<h4 class="anchored" data-anchor-id="pros-and-cons-of-synthetic-and-hybrid-microdata">Pros and cons of synthetic and hybrid microdata</h4>
<p>Synthetic data are appealing in that, at a first glance, they seem to circumvent the re-identification problem: since published records are invented and do not derive from any original record, it might be concluded that no individual can complain from having been re-identified. At a closer look this advantage is less clear. If, by chance, a published synthetic record matches a particular citizen’s non-confidential variables (age, marital status, place of residence, etc.) and confidential variables (salary, mortgage, etc.), re-identification using the non-confidential variables is easy and that citizen may feel that his confidential variables have been unduly revealed. In that case, the citizen is unlikely to be happy with or even understand the explanation that the record was synthetically generated.</p>
<p>On the other hand, limited data utility is another problem of synthetic data. Only the statistical properties explicitly captured by the model used by the data protector are preserved. A logical question at this point is why not directly publish the statistics one wants to preserve rather than release a synthetic microdata set.</p>
<p>One possible justification for synthetic microdata would be if valid analyses could be obtained on a number of subdomains, <em>i.e.</em> similar results were obtained in a number of subsets of the original dataset and the corresponding subsets of the synthetic dataset. Partially synthetic or hybrid microdata are more likely to succeed in staying useful for subdomain analysis. However, when using partially synthetic or hybrid microdata, we lose the attractive feature of purely synthetic data that the number of records in the protected (synthetic) dataset is independent from the number of records in the original dataset.</p>
</section>
<section id="references-3" class="level4">
<h4 class="anchored" data-anchor-id="references-3">References</h4>
<p>Abowd, J. M., and Woodcock, S. D. (2001). <em>Disclosure limitation in longitudinal linked tables</em>. In P.&nbsp;Doyle, J.&nbsp;I. Lane, J.&nbsp;J. Theeuwes, and L.&nbsp;V. Zayatz, editors, Confidentiality, Disclosure and Data Access: Theory and Practical Applications for Statistical Agencies, pages 215–278, Amsterdam, 2001. North-Holland.</p>
<p>Abowd, J. M. and Woodcock, S. D. (2004). <em>Multiply-imputing confidential characteristics and file links in longitudinal linked data</em>. In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 290–297, Berlin Heidelberg, 2004. Springer.</p>
<p>Aggarwal, C. C., and Yu, P. S. (2004). <em>A condensation approach to privacy preserving data mining</em>. In E. Bertino, S. Christodoulakis, D. Plexousakis, V. Christophides, M. Koubarakis, K. Böhm, and E. Ferrari, editors, Advances in Database Technology - EDBT 2004, volume 2992 of Lecture Notes in Computer Science, pages 183–199, Berlin Heidelberg, 2004.</p>
<p>Burridge, J. (2004). <em>Information preserving statistical obfuscation.</em> Statistics and Computing, 13:321–327, 2003.</p>
<p>Crystal.Ball. <a href="http://www.cbpro.com/"><u>http://www.cbpro.com/</u></a>.</p>
<p>Dandekar, R., Cohen, M., and Kirkendall, N. (2002). <em>Sensitive micro data protection using latin hypercube sampling technique.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 245–253, Berlin Heidelberg, Springer.</p>
<p>Dandekar, R., Domingo-Ferrer, J., and Sebé, F. (2002). <em>LHS-based hybrid microdata vs.&nbsp;rank swapping and microaggregation for numeric microdata protection.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 153–162, Berlin Heidelberg. Springer.</p>
<p>Domingo-Ferrer, J., and González-Nicolás, Ú. (2009). <em>Hybrid Microdata Using Microaggregation</em>. Manuscript.</p>
<p>Fienberg, S. E. (1994). <em>A radical proposal for the provision of micro-data samples and the preservation of confidentiality</em>. Technical Report 611, Carnegie Mellon University Department of Statistics.</p>
<p>Fienberg, S.E., Makov, U. E., and Steele, R. J. (1998). <em>Disclosure limitation using perturbation and related methods for categorical data</em>. Journal of Official Statistics, 14(4):485–502.</p>
<p>Florian, A. (1992). <em>An efficient sampling scheme: updated latin hypercube sampling</em>. Probabilistic Engineering Mechanics, 7(2):123–130.</p>
<p>Franconi, L., and Stander, J. (2002). <em>A model based method for disclosure limitation of business microdata</em>. Journal of the Royal Statistical Society D - Statistician, 51:1–11.</p>
<p>Huntington, D. E., and Lyrintzis, C. S. (1998). <em>Improvements to and limitations of latin hypercube sampling</em>. Probabilistic Engineering Mechanics, 13(4):245–253.</p>
<p>Kennickell, A. B. (1999). <em>Multiple imputation and disclosure control: the case of the 1995 survey of consumer finances.</em> In Record Linkage Techniques, pages 248–267, Washington DC, 1999. National Academy Press.</p>
<p>Kennickell, A. B. (1999b). <em>Multiple imputation and disclosure protection: the case of the 1995 survey of consumer finances.</em> In J.&nbsp;Domingo-Ferrer, editor, Statistical Data Protection, pages 248–267, Luxemburg, 1999. Office for Official Publications of the European Communities.</p>
<p>Liew, C. K., Choi, U. J., and Liew, C. J. (1985). <em>A data distortion by probability distribution.</em> ACM Transactions on Database Systems, 10:395–411, 1985.</p>
<p>Mateo-Sanz, J. M., Martínez-Ballesté, A., and Domingo-Ferrer, J. (2004). <em>Fast generation of accurate synthetic microdata.</em> In J.&nbsp;Domingo-Ferrer and V.&nbsp;Torra, editors, Privacy in Statistical Databases, volume 3050 of LNCS, pages 298–306, Berlin Heidelberg, Springer.</p>
<p>Muralidhar, K, and Sarathy, R, (2008). <em>Generating sufficiency-based nonsynthetic perturbed data</em>. Transactions on Data Privacy, 1(1):17–33, 2008. <a href="http://www.tdp.cat/issues/tdp.a005a08.pdf"><u>http://www.tdp.cat/issues/tdp.a005a08.pdf</u></a>.</p>
<p>Polettini, S., Franconi, L., and Stander, J. (2002). <em>Model based disclosure protection.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 83–96, Berlin Heidelberg. Springer.</p>
<p>Raghunathan, T. J., Reiter, J. P., and Rubin, D. (2003). <em>Multiple imputation for statistical disclosure limitation.</em> Journal of Official Statistics, 19(1):1–16.</p>
<p>Reiter, J. P. (2002). Satisfying disclosure restrictions with synthetic data sets. <em>Journal of Official Statistics</em>, 18(4):531–544.</p>
<p>Reiter, J. P. (2003). Inference for partially synthetic, public use microdata sets. <em>Survey Methodology</em>, 29:181–188.</p>
<p>Reiter, J. P. (2003b). <em>Using CART to generate partially synthetic public use microdata, 2003</em>. Duke University working paper.</p>
<p>Reiter, J. P. (2005). <em>Releasing multiply-imputed, synthetic public use microdata: An illustration and empirical study.</em> Journal of the Royal Statistical Society, Series A, 168:185–205.</p>
<p>Reiter, J. P. (2005b). <em>Significance tests for multi-component estimands from multiply-imputed, synthetic microdata</em>. Journal of Statistical Planning and Inference, 131(2):365–377.</p>
<p>Rubin, D. E. (1993). <em>Discussion of statistical disclosure limitation.</em> Journal of Official Statistics, 9(2):461–468.</p>
<p>Sebé, F., Domingo-Ferrer, J., Mateo-Sanz, J. M. and Torra, V. (2002). <em>Post-masking optimization of the tradeoff between information loss and disclosure risk in masked microdata sets.</em> In J.&nbsp;Domingo-Ferrer, editor, Inference Control in Statistical Databases, volume 2316 of LNCS, pages 163–171, Berlin Heidelberg, Springer.</p>
</section>
</section>
<section id="targeted-record-swapping" class="level3">
<h3 class="anchored" data-anchor-id="targeted-record-swapping">Targeted Record Swapping</h3>
<p>Targeted Records Swapping (TRS) is a pre-tabular perturbation method. It’s intended use is to apply a swapping procedure to the micro data before generating a table. Although it is applied solely on micro data it is generally considered a protection method used for tabular data and not recommended for protecting micro data. TRS can be used for tables with and without spatial characteristics, with the prior case containing also grid data products or tables created by cross-tabulating with grid cells.<br>
During the TRS the spatial character of the data can be taken into account to some degree.</p>
<section id="the-trs-noise-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="the-trs-noise-mechanism">The TRS noise mechanism</h4>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Expert level</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Regardless of the table, be it count data or a magnitude table, the methodology of the TRS does not change. This is a direct consequence of the fact that the method is applied to the underlying micro data before generating any table.</p>
<p>Consider population units <span class="math inline">\(i = 1, \ldots, N\)</span> where each unit <span class="math inline">\(i\)</span> has <span class="math inline">\(p\)</span> characteristics or variables <span class="math inline">\(\{x\}_{i,j} = \mathbf{X} \in \mathbb{R}^{n\times p}\)</span>. Furthermore there exists a geographic hierarchy <span class="math inline">\(\mathcal{G}^{1} \succ \mathcal{G}^{2} \succ \ldots \succ \mathcal{G}^{K}\)</span> where each <span class="math inline">\(\mathcal{G}^{k}\)</span> is the set of disjointly split areas <span class="math inline">\(g_m^{k}\)</span>, <span class="math inline">\(m=1,\ldots,M_k\)</span> and each <span class="math inline">\(g_m^{k}\)</span> is further disjointly subdivided into smaller areas <span class="math inline">\(g_m^{k+1}\)</span>,<span class="math inline">\(m=1,\ldots,M_{k+1}\)</span>:</p>
<p><span class="math display">\[
\mathcal{G}^{k} = \{g_m^{k} \mid g_i^{k}\cap g_j^{k} = \varnothing \text{ for }i\neq j \} \quad \forall k = 1,\ldots,K
\]</span></p>
<p>where</p>
<p><span class="math display">\[
g_m^{k} = \dot{\bigcup}_{m=1}^{M_{k+1}}g_m^{k+1} \quad \forall k = 1,\ldots,K-1 \quad .
\]</span></p>
<p>Each unit <span class="math inline">\(i\)</span> in the population can be assigned to a single area <span class="math inline">\(g_{m_i}^{k}\)</span> for each geographic hierarchy level <span class="math inline">\(\mathcal{G}^{k}\)</span>, <span class="math inline">\(k = 1,\ldots,K\)</span>. Consider as geographic hierarchy for example the NUTS regions, NUTS1 <span class="math inline">\(\succ\)</span> NUTS2 <span class="math inline">\(\succ\)</span> NUTS3, or grid cells, 1000m grid cells <span class="math inline">\(\succ\)</span> 500m grid cells <span class="math inline">\(\succ\)</span> 250m grid cells.</p>
<p>Calculate for each unit <span class="math inline">\(i = 1, \ldots, N\)</span> risk values <span class="math inline">\(r_{i,k}\)</span> on each geographic hierarchy level <span class="math inline">\(\mathcal{G}^{k}\)</span>, <span class="math inline">\(k = 1,\ldots,K\)</span>, given a specified set of variables <span class="math inline">\(\mathbf{x}_{q_1},\ldots,\mathbf{x}_{q_Q}\)</span>. As an example one can choose <span class="math inline">\(k\)</span>-anonymity as risk measure to derive risk values <span class="math inline">\(r_{i,k}\)</span>. They can be defined by calculating the number of units <span class="math inline">\(j\)</span> which have the same values for variables <span class="math inline">\(\mathbf{x}_{q_1},\ldots,\mathbf{x}_{q_Q}\)</span> as unit <span class="math inline">\(i\)</span> and taking the inverse.</p>
<p><span class="math display">\[
c_{i,k} = \sum\limits_{j=1}^N \mathbf{1}[x_{i,q_1} = x_{j,q_1}, x_{i,q_2} = x_{j,q_2}, \ldots, x_{i,q_Q} = x_{j,q_Q}]
\]</span></p>
<p><span class="math display">\[
r_{i,k} = \frac{1}{c_{i,k}}
\]</span></p>
<p>Having the risk values <span class="math inline">\(r_{i,k}\)</span> for each unit <span class="math inline">\(i\)</span> and each geographic hierarchy level calculated the TRS can be defined as follows:</p>
<ol type="1">
<li>Define initial, use-case specific, parameter.
<ul>
<li>A global swap rate <span class="math inline">\(p\)</span></li>
<li>Define a risk value <span class="math inline">\(r_{high}\)</span> beyond which all units with <span class="math inline">\(r_{i,k}\)</span> are considered for the geographic hierarchy level <span class="math inline">\(k\)</span>.</li>
<li>A set of variables <span class="math inline">\(\mathbf{x}_{t_1},\ldots,\mathbf{x}_{t_T}\)</span> which are considered while swapping units</li>
</ul></li>
<li>Begin at the first hierarchy level <span class="math inline">\(\mathcal{G}^{1}\)</span> and select all units <span class="math inline">\(j\)</span> for which <span class="math inline">\(r_{i,1} \geq r_{high}\)</span>.</li>
<li>For each <span class="math inline">\(j\)</span> select all units <span class="math inline">\(l_1,\ldots,l_L\)</span>, which do not belong to the same area <span class="math inline">\(g_{m_j}^{1}\)</span> and have the same values for variables <span class="math inline">\(\mathbf{x}_{t_1},\ldots,\mathbf{x}_{t_T}\)</span> as unit <span class="math inline">\(j\)</span>. In addition units <span class="math inline">\(l_1,\ldots,l_L\)</span> cannot have been swapped already. <span class="math display">\[
g_{m_j}^{1} \neq g_{m_l}^{1}
\]</span> <span class="math display">\[
x_{j,t_1} = x_{l,t_1}, x_{j,t_2} = x_{l,t_2}, \ldots, x_{j,t_T} = x_{l,t_T}
\]</span></li>
<li>Sample for each <span class="math inline">\(j\)</span> one unit from the set <span class="math inline">\(\{l \mid g_{m_j}^{1} \neq g_{m_l}^{1} \land x_{j,t_1} = x_{l,t_1} \land, \ldots, \land x_{j,t_T} = x_{l,t_T}\}\)</span> by normalising corresponding risk value <span class="math inline">\(r_{l,1}\)</span> and using them as sampling probabilities.
<ul>
<li>Previously swapped units should be excuded from this set.</li>
</ul></li>
<li>Swap all variables, holding geographic information in <span class="math inline">\(\mathbf{X}\)</span>, between unit <span class="math inline">\(j\)</span> and the sampled unit.
<ul>
<li>Some implementation of targeted record swapping consider only swapping specific variable values from <span class="math inline">\(\mathbf{X}\)</span> between <span class="math inline">\(j\)</span> and the sampled unit.</li>
</ul></li>
<li>Iterate through the geographic hierarchies <span class="math inline">\(k=2,\ldots,K\)</span> and repeat in each of them steps 3. - 5.</li>
<li>At the final geographic hierarchy <span class="math inline">\(k=K\)</span> if the number of already swapped units is less than <span class="math inline">\(p\times N\)</span> additional units are swapped to reach <span class="math inline">\(p\times N\)</span> overall swaps.</li>
</ol>
<p>If the population units refer to people living in dwellings and the aim is to swap only full dwellings with each other and not only individuals it can be useful to set</p>
<p><span class="math display">\[
r_{i,k} = \max_{j \text{ living in same dwelling as }i}r_{j,k}
\]</span></p>
<p>prior to applying the swapping procedure. In addition <span class="math inline">\(\mathbf{x}_{1(q)},\ldots,\mathbf{x}_q\)</span> should be defined such that they refer to variables holding dwelling information.</p>
The above described procedure is implemented in the R package <code>sdcMicro</code> as well as in the software <code>muArgus</code>, alongside a multitude of parameters to fine tune the procedure.
</blockquote>
</section>
<section id="pros-and-cons-of-targeted-record-swapping" class="level4">
<h4 class="anchored" data-anchor-id="pros-and-cons-of-targeted-record-swapping">Pros and cons of targeted record swapping</h4>
<p>Indicated by the name of the method the TRS aims to swap micro data records prior to building a table and specifically targeting records during the swapping procedure which have a higher risk of disclosure with respect to the final tables. The protection of the TRS itself is considered to be the uncertainty that an identified unit <span class="math inline">\(i\)</span> has a considerable chance of actually being a swapped unit and that the information derived from this unit does not contain the information of the original unit <span class="math inline">\(i\)</span>. In general it is recommended to apply the TRS on the micro data set only once and afterwards build various tables from the same perturbed micro data. This creates a more drastic trade off between the number of records to swap and the utility of the final tables. The swapping procedure can indirectly take into account the structure of the final tables through the risk value derived from the variables <span class="math inline">\(\mathbf{x}_{q_1},\ldots,\mathbf{x}_{q_Q}\)</span> and choice of the geographic hierarchy. However a large number of variables <span class="math inline">\(\mathbf{x}_{q_1},\ldots,\mathbf{x}_{q_Q}\)</span> and a high resolution in the geographic hierarchy can result in a high share of units with high risk values and consequently many potential swaps. A high swap rate, for instance beyond 10%, can quickly lead to high information loss, because the noise introduced through the swapping is not controlled for while drawing the swapped units. Thus it is not feasible to both address all possible disclosure risk scenarios while maintaining high utility in the final tables.</p>
<p>As with any method it is advised to thoroughly tune parameters to balance information loss disclosure risk. Possible tuning parameters are:</p>
<ul>
<li>The geographic hierarchy and its depth of granularity.</li>
<li>The construction of the risk values <span class="math inline">\(r_{i,k}\)</span> and <span class="math inline">\(r_{high}\)</span></li>
<li>The choice of <span class="math inline">\(\mathbf{x}_1,\ldots,\mathbf{x}_s\)</span></li>
<li>The global swap rate <span class="math inline">\(p\)</span></li>
</ul>
<p>Because the noise is applied to the micro data before building any tables the additivity between inner and marginal aggregates will always be preserved.</p>
</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>When <span class="math inline">\(X\)</span> has <span class="math inline">\(K\)</span> categories and <span class="math inline">\(Y\)</span> has <span class="math inline">\(L\)</span> categories, the 2-dimensional frequency table <span class="math inline">\(T_{XY}\)</span> is a <span class="math inline">\(K\times L\)</span> matrix.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>