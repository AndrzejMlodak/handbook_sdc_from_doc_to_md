## Measurement of disclosure risk and information loss 
### Disclosure risk

The disclosure risk for tabular data (especially magnitude) includes the assessment of: 
• primary risk,
• secondary risk.
The primary risk concerns the threat for direct identification of an unit resulting from too low frequency or existong of outliers according to the presented magnitude in the cell. The secondary risk assessment is necessary due to the fact that primary confidential cells in detailed tables may still not ensure sufficient protection against re-identification: together with single cells also sums for larger groups, i.e. the margins are computed. Then the protection of the primary sensitive cells can be undone, by some differencing. Therefore the risk of such an event should also be assessed. 

The key measure of dislosure risk in the case of magnitude tables is based on the $(n,l)$-dominance or $p\%$ rules. In many formal regulations or recommendations it is assumed $n=1$ and $l=75$. Very often $(n,l)$-dominance is combined with $k$-anonymity: a cell is regarded as unsafe if it violates the $k$-anonymity or the $(n,l)$-dominance. Therefore the dislosure risk can be measured as the share of cells violating the finally assumed principle. A broader discussion on an application of these rules to assess the disclosure risk is performed e.g. by Hundepool et al. (2012).

However, this assessment concerns only the primary risk. One should take into account the risk of secondary re-identification of unit. Antal, Shlomo and Elliot (2014), formulate four fundamental properties which should have an efficient disclosure risk measure: 
• small cell values should have higher disclosure risk than larger,
• uniformly distributed frequencies imply low disclosure risk,
• the more zero cells in the census table, the higher the disclosure risk,
• the risk measure should be bounded by 0 and 1.

In the currently investigated case of magnitude table one should add one more condition concerning the specificity of this type of data presentation. That is, the larger is the share of the largest contributors in a cell the higher is the disclosure risk.

Shlomo, Antal and Elliot (2015) proposed in this context measure based on the entropy. According to their approach, a high entropy indicates that the distribution across cells is uniform and a low entropy indicates mainly zeros in a row/column or table with a few non-zero cells. The fewer the number of non-zero cells, the more likely that attribute disclosure occurs. The entropy is given as
$$
H=-\sum_{i=1}^{k}{\frac{c_i}{n}\cdot\log{\frac{c_i}{n}}}, (4.6.1)
$$
where $c_i$ is the number of units contained in $i$-th cell and $n$ is the total number of cells in the table. The measure (4.6.1) is next normalized to the form
$$
1-\frac{H}{\log n}. (4.6.2)
$$

This approach, however, doesn't take our fifth condition into account. So, one can correct (4.6.2) to the form combining both aspects, e.g. in the following way:     
$$
r=1-\frac{1}{2}\left(\frac{H}{\log n}+\frac{1}{n}\sum_{i=1}^n{\frac{x_{i(\text{max})}}{\sum_{j\in C_i}{x_j}}}\right),
$$
where $x_{i(\text{max})}$ is the share of the largest contributors to the cell $C_i$ and $x_j$ is the contribution of the $j$-th unit to it. This measure takes value from $[0,1]$ and is easy interpretable.    

### Information loss

### References
Antal, L., Shlomo, N., & Elliot, M. (2014). Measuring disclosure risk with entropy in population based frequency tables. In Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings (pp. 62-78). Springer International Publishing.
Shlomo, N., Antal, L., & Elliot, M. (2015). Measuring disclosure risk and data utility for flexible table generators. *Journal of Official Statistics*, 31(2), 305-324.
