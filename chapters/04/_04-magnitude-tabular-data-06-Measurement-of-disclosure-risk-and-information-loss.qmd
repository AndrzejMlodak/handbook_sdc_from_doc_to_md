## Measurement of disclosure risk and information loss 
### Disclosure risk

The disclosure risk for tabular data (especially magnitude) includes the assessment of: 
• primary risk,
• secondary risk.
The primary risk concerns the threat for direct identification of an unit resulting from too low frequency or existong of outliers according to the presented magnitude in the cell. The secondary risk assessment is necessary due to the fact that primary confidential cells in detailed tables may still not ensure sufficient protection against re-identification: together with single cells also sums for larger groups, i.e. the margins are computed. Then the protection of the primary sensitive cells can be undone, by some differencing. Therefore the risk of such an event should also be assessed. 

The key measure of dislosure risk in the case of magnitude tables is based on the $(n,l)$-dominance or $p\%$ rules. In many formal regulations or recommendations it is assumed $n=1$ and $l=75$. Very often $(n,l)$-dominance is combined with $k$-anonymity: a cell is regarded as unsafe if it violates the $k$-anonymity or the $(n,l)$-dominance. Therefore the dislosure risk can be measured as the share of cells violating the finally assumed principle. A broader discussion on an application of these rules to assess the disclosure risk is performed e.g. by Hundepool et al. (2012).

However, this assessment concerns only the primary risk. One should take into account the risk of secondary re-identification of unit. Antal, Shlomo and Elliot (2014), formulate four fundamental properties which should have an efficient disclosure risk measure: 
• small cell values should have higher disclosure risk than larger,
• uniformly distributed frequencies imply low disclosure risk,
• the more zero cells in the census table, the higher the disclosure risk,
• the risk measure should be bounded by 0 and 1.

In the currently investigated case of magnitude table one should add one more condition concerning the specificity of this type of data presentation. That is, the larger is the share of the largest contributors in a cell the higher is the disclosure risk.

Shlomo, Antal and Elliot (2015) proposed in this context measure based on the entropy. According to their approach, a high entropy indicates that the distribution across cells is uniform and a low entropy indicates mainly zeros in a row/column or table with a few non-zero cells. The fewer the number of non-zero cells, the more likely that attribute disclosure occurs. The entropy is given as
$$
H=-\sum_{i=1}^{k}{\frac{c_i}{n}\cdot\log{\frac{c_i}{n}}}, (4.6.1)
$$
where $c_i$ is the number of units contained in $i$-th cell and $n$ is the total number of cells in the table. The measure (4.6.1) is next normalized to the form
$$
1-\frac{H}{\log n}. (4.6.2)
$$

This approach, however, doesn't take our fifth condition into account. So, one can correct (4.6.2) to the form combining both aspects, e.g. in the following way:     
$$
r=1-\frac{1}{2}\left(\frac{H}{\log n}+\frac{1}{n}\sum_{i=1}^n{\frac{x_{i(\text{max})}}{\sum_{j\in C_i}{x_j}}}\right),
$$
where $x_{i(\text{max})}$ is the share of the largest contributors to the cell $C_i$ and $x_j$ is the contribution of the $j$-th unit to it. This measure takes value from $[0,1]$ and is easy interpretable. 

The disclosure risk assessment can be also adjusted to the specificity of currently used SDC method. For instance, Enderle, Giessing and Tent (2020) have propsed an original risk measure for continous data perturbed using the Cell Key approach. It assumes that an intruder can know the noise parameter and the maximum deviation parameter. Then he/she can determine the feasibility intervals for original internal and margin cells using the noisy values. The risk estimate is then based on the sum of products of cumulated probabilities resulting from entries of p-table and empirical probabilities for ratio of noise parameters.     

### Information loss

The basis of assesment of information loss for tables are differences between values of cells determined using perturbed microdata (or perturbed during creation of  tables) and relevant values obtained on the basis of the original data. Niech $T_0$ oznacza tablicę powstałą na podstawie mikrodanych oryginalnych, a~$T_1$ -- analogiczną tablicę stworzoną w~oparciu o~odpowiednie mikrodane poddane SDC przy użyciu metod zakłóceniowych (lub w~której same wartości komórek zostały zakłócone). $T_l(c)$ oznaczać będzie komórkę $c$ tablicy $T_l$, $l=0,1$.

Do określenia miar straty informacji rekomenduje się powszechnie stosowanie następujących metryk:
\begin{itemize}
\item odchylenie bezwzględne: $|T_1(c)-T_0(c)|$,
\item relatywne odchylenie bezwzględne: $\frac{|T_1(c)-T_0(c)|}{T_0(c)}$, 
\item bezwzględne odchylenie pierwiastków kwadratowych: $|\sqrt{T_1(c)}-\sqrt{T_0(c)}|$.
\end{itemize}

Jak można zauważyć, bezwzględne odchylenie pierwiastków kwadratowych ma w~praktyce sens tylko wtedy, gdy wartości komórek w~tablicach są nieujemne. Ta sytuacja ma jednak miejsce najczęściej.

Wykorzystując podane metryki można określić kompleksowe miary straty informacji dla określonego agregatu $A$ (może nim być cała tablica, ale może to być też określony podzbiór komórek tablicy odnoszący się np. do tej samej jednostki przestrzennej -- jak choćby liczby ludności według grup wieku dla powiatu). Pierwszą z~tych miar jest przeciętne odchylenie bezwzględne (ang. \textit{absolute deviation}) -- średnia z~bezwzględnych różnic pomiędzy częstościami w~tablicach oryginalnych oraz po zastosowaniu SDC:

\begin{equation}
\textmd{AD}(A)=\frac{\sum_{c\in A}{|T_1(c)-T_0(c)|}}{n_A},
\label{ad}
\end{equation}
przy czym $n_A$ oznacza liczbę komórek objętych agregatem $A$.

Można też wyznaczać sumę relatywnych odchyleń bezwzględnych (ang. \textit{relative absolute deviation}), czyli sumę względnych różnic pomiędzy częstościami w~tablicach):
\begin{equation}
\textmd{RAD}(A)=\sum_{c\in A}{\frac{|T_1(c)-T_0(c)|}{T_0(c)}}.
\label{rad}
\end{equation}

Ostatnią z~prezentowanych tutaj miar jest miara oparta na bezwzględnych różnicach pierwiastków kwadratowych z~częstości w~tablicach, wykorzystująca formułę odległości Hellingera\footnote{Zaproponowaną w~1909 r. przez niemieckiego matematyka Ernsta Hellingera (1883--1950).} (ang. \textit{absolute difference of square roots -- Hellinger’s Distance}):
\begin{equation}
\textmd{HD}(A)=\sqrt{\frac{1}{2}\sum_{c\in A}{\left(\sqrt{T_1(c)}-\sqrt{T_0(c)}\right)^2}}.
\label{hd}
\end{equation}

W~tablicach mogą wszakże występować braki danych. W~przypadku metod zakłóceniowych braki te będą wynikać przede wszystkim z~braków występujących w~oryginalnych mikrodanych stanowiących podstawę konstrukcji tablic. Wówczas podczas wyznaczania wartości w~komórkach można owe braki pominąć lub wcześniej dokonać imputacji danych dla tychże luk. Jedyny kłopot może występować w~przypadku, gdy nie będzie żadnych danych dotyczących kategorii wyznaczonej daną komórką. Wtedy trzeba albo zrezygnować z~danej konstrukcji tablicy (np. poprzez połączenie pewnych jej kategorii w~inną, bardziej zgrubną), albo też miarę straty oprzeć na pomiarze straty na poziomie mikrodanych odpowiadających tejże komórce dokonanym w~sposób opisany we wcześniejszych paragrafach.
 
Trudniej przedstawia się sprawa, gdy stosuje się zakłóceniowe metody SDC, co prowadzi do ukrywania wartości pewnych komórek. W~takim przypadku do porównań trzeba zasymulować tablicę, w~której dla brakujących komórek ich wartości zostaną zaimputowane. W~przypadku ukrywania komórek strata informacji zostanie wyrażona jako suma kosztów poniesionych wskutek wtórnego ukrycia komórek. Problemem pozostaje to, czy waga każdej komórki w~tablicy jest taka sama, czy też komórki o~większej wartości mają wagę większą. W~praktyce ukrycie zbyt dużej liczby komórek o~wysokich wartościach może znacznie obniżyć użyteczność publikowanych danych. W~zależności od preferencji i~potrzeb użytkowników zagadnienie straty informacji może być różnie wyrażone. W~ten sposób da się wpłynąć na działanie algorytmu wyboru komórek do wtórnego ukrycia. \cite{Hund12} wskazują najbardziej popularne kryteria brane pod uwagę przy formułowaniu funkcji kosztu dla ukrywania komórek:
\begin{itemize}
\item jednakowa waga dla wszystkich komórek -- celem jest minimalizacja liczby wtórnie ukrytych komórek,
\item liczba jednostek w~agregacie, który komórka reprezentuje -- prowadzi do poszukiwania możliwości ukrycia tylko takich komórek, które łącznie będą reprezentować jak najmniejszą liczbę jednostek,
\item wartość komórki -- optymalnym rozwiązaniem będzie pozostawienie w~publikacji jak największej liczby komórek o~największych wartościach. 
\end{itemize}

W~sytuacji występowania silnej asymetrii danych preferowanie zachowania komórek o~największej wartości może prowadzić do zbyt dużej nierównowagi dla funkcji kosztu. W~tym wypadku zalecana jest transformacja funkcji kosztu. Jednym z~możliwych i~często stosowanych podejść w~tym zakresie jest transformacja potęgowa (zob. \cite{BC64}):
\[
y=\begin{cases}
x^\lambda&\textmd{dla}\;\lambda\ne 0,\cr
\log(x)&\textmd{dla}\;\lambda=0.
\end{cases}
\]

### References
Antal, L., Shlomo, N., & Elliot, M. (2014). Measuring disclosure risk with entropy in population based frequency tables. In Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings (pp. 62-78). Springer International Publishing.

Enderle, T., Giessing, S., & Tent, R. (2020). Calculation of risk probabilities for the cell key method. In Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2020, Tarragona, Spain, September 23–25, 2020, Proceedings (pp. 151-165). Springer International Publishing.

Shlomo, N., Antal, L., & Elliot, M. (2015). Measuring disclosure risk and data utility for flexible table generators. *Journal of Official Statistics*, 31(2), 305-324.
