## Measurement of dislosure risk and information loss

### Disclosure risk
As described in Section 5.3 and 5.4 there are a number of different disclosure control methods used to protect frequency tables. Each of these methods modifies the original data in the table in order to reduce the disclosure risk from small cells (e.g. 0's, 1's and 2's). The fundament of assessment of disclosure risk in this case it then the $k$-anonymity rule. Most often it is assumed that $k=3$, although sometimes also $k=2$ or $k=5$ is applied. So, in the frequency tables the primary risk results only for direct identification of an unit resulting from too low frequency and the secondary risk assessment is connected with possibility of deduction of suppressed/perturbed cells by relevant differenciation of table. 

Similarly as in the case of magnitude tables (see Section 4.6) a cell is regarded as unsafe if it violates the $k$-anonymity. Therefore the dislosure risk can be measured as the share of cells having this property in the total number of cells in a table.

As regards the secondary risk, its measure should satisfy four conditions formulated by Antal, Shlomo and Elliot (2014) and recalled in Section 4.6. Then the measure (4.6.1) and (4.6.2) can be applied directly in this case.

Denote the population based frequency table by 𝐹 = (𝐹1, 𝐹2, … , 𝐹𝐾). It implies that the
size of the frequency table is 𝐾. Assume that the distribution of 𝑋 is (
𝐹1
𝑁
,
𝐹2
𝑁
, … ,
𝐹𝐾
𝑁
),
where 𝑁 = ∑ 𝐹𝑖
𝐾
𝑖=1
. The disclosure risk for a population based frequency table is a
weighted average defined as follows.
𝑅1
(𝐹, 𝒘) = 𝑤1 ∙
|𝐷|
𝐾
+ 𝑤2 ∙ (1 −
𝐻(𝑋)
log 𝐾
) − 𝑤3 ∙
1
√𝑁
∙ log
1
𝑒 ∙ √𝑁
(1)
Here 𝐷 denotes the set of zeroes in the table, therefore |𝐷| provides the number of
zeroes. 𝒘 = (𝑤1, 𝑤2, 𝑤3
) is the vector of weights, while 𝑒 is the base of the natural
logarithm. The disclosure risk measure reflects the properties listed in section 1.
If a statistical agency deem that a population based frequency table is of high disclosure
risk, then a statistical disclosure control (SDC) method must be applied to the table.
Denote the perturbed frequency table by 𝐺 = (𝐺1,𝐺2, … , 𝐺𝐾). The disclosure risk of the
perturbed table also must be assessed. However, the previously defined disclosure risk
measure cannot be applied to the perturbed table since the perturbed table has more
uncertainty. We define the disclosure risk after perturbation below.
𝑅2
(𝐹, 𝐺, 𝒘) = 𝑤1 ∙ (
|𝐷|
𝐾
)
|𝐷∪𝐸|
|𝐷∩𝐸|
+ 𝑤2 ∙ (1 −
𝐻(𝑋)
log 𝐾
) ∙ (1 −
𝐻(𝑋|𝑌)
𝐻(𝑋)
)
− 𝑤3 ∙
1
√𝑁
∙ log
1
𝑒 ∙ √𝑁
(2)
Here 𝐸 is the set of zeroes in the perturbed frequency table. The disclosure risk of a
perturbed table is expected to be lower than that of the original table. 𝑅2
(𝐹, 𝐺, 𝒘)
satisfies this requirement.

### Information loss
However, the process of reducing disclosure risk results in information loss. Some quantitative information loss measures have been developed by Shlomo and Young (2005 & 2006) to determining the impact various statistical disclosure control (SDC) methods have on the original tables.

Information loss measures can be split into two classes; measures for data suppliers used to make informed decisions about optimal SDC methods depending on the characteristics of the tables; and measures for users in order to facilitate adjustments to be made when carrying out statistical analysis on protected tables. Here we focus on measures for data suppliers. Measuring utility and quality for SDC methods is subjective. It depends on the users; the purpose of the data and the required statistical analysis; and the type and format of the statistical data. Therefore it is useful to have a range of information loss measures for assessing the impact of the SDC methods.

The focus here is information loss measures for tables containing frequency counts, however, some of these measures can easily be adapted to microdata. Magnitude or weighted sample tables will have the additional element of the number of contributors to each cell of the table.

When evaluating information loss measures for tables protected using cell suppression, one needs to decide on an imputation method for replacing the suppressed cells similar to what one would expect a user to do prior to analysing the data, (i.e. we need to measure the difference between the observed and actual values, and for suppressed cells the observed values will be based on user inference about the possible cell values). A naive user might use zeros in place of the suppressed cells whereas a more sophisticated user might replace suppressed cells by some form of averaging of the total information that was suppressed, or by calculating feasibility intervals.

A number of different information loss measures are described below, and more technical details can be found in Shlomo and Young (2005 & 2006).

-   An exact Binomial Hypothesis Test can be used to check if the realization of a random stochastic perturbation scheme, such as random rounding, follows the expected probabilities (i.e. the parameters of the method). For other SDC methods, a non-parametric signed rank test can be used to check whether the location of the empirical distribution after the application of the SDC method, has changed.

-   Information loss measures that measure distortion to distributions are based on distance metrics between the original and perturbed cells. Some useful metrics are also presented in Gomatam and Karr (2003). A distance metric can be calculated for internal cells of a table. When combining several tables one may want to calculate an overall average across the tables as the information loss measure. These distance metrics can also be calculated for totals or sub-totals of the tables.

-   SDC methods will have an impact on the variance of the average cell size for the rows, columns or the entire table. The variance of the average cell size is examined before and after the SDC method has been applied. Another important variance to examine is the "between" variance when carrying out a one-way ANOVA test based on the table. In ANOVA, we examine the means of a specific target variable within groupings defined by independent categorical variables. The goodness of fit statistic R^2^ for testing the null hypothesis that the means are equal across the groupings is based on the variance of the means between the groupings divided by the total variance. The information loss measure therefore examines the impact of the "between" variance and whether the means of the groupings have become more homogenized or spread apart as a result of the SDC method.

-   Another statistical analysis tool that is frequently carried out on tabular data are tests for independence between categorical variables that span the table. The test for independence for a two-way table is based on a Pearson Chi-Squared Statistic and the measure of association is the Cramer's V statistic. For multi-way tables, one can examine conditional dependencies and calculate expected cell frequencies based on the theory of log-linear models. The test statistic for the fit of the model is also based on a Pearson Chi-Squared Statistic. SDC methods applied to tables may change the results of statistical inferences. Therefore we examine the impact to the test statistics before and after the application of the SDC method.

-   Another statistical tool for inference is the Spearman's Rank Correlation. This is a technique that tests the direction and strength of the relationship between two variables. The statistic is based on ranking both sets of data from the highest to the lowest. Therefore, one important assessment of the impact of the SDC method on statistical data is whether we are distorting the rankings of the cell counts.

In order to allow data suppliers to make informed decisions about optimal disclosure control methods, ONS have developed a user-friendly software application that calculates both disclosure risk measures based on small counts in tables and a wide range of information loss measures (as described above) for disclosure controlled statistical data, Shlomo and Young (2006). The software application also outputs R-U Confidentiality maps.

### References
Antal, L., Shlomo, N., & Elliot, M. (2014), Measuring disclosure risk with entropy in population based frequency tables. In *Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings* (pp. 62-78). Springer International Publishing.

Antal, L., & Shlomo, N., & Elliot, M. (2015), Disclosure Risk Measurement with Entropy in Sample Based Frequency Tables, *New Techniques and Technologies for Statistics (NTTS) Reliable Evidence for a Society in Transition, Brussels, Belgium, 9-13 March 2015*, https://cros-legacy.ec.europa.eu/system/files/Antal-etal_NTTS%202015%20abstract%20unblinded%20disclosure%20risk%20measurement.pdf.
