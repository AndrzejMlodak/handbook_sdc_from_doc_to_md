## Measurement of dislosure risk and information loss

### Disclosure risk
As described in Section 5.3 and 5.4 there are a number of different disclosure control methods used to protect frequency tables. Each of these methods modifies the original data in the table in order to reduce the disclosure risk from small cells (e.g. 0's, 1's and 2's). The fundament of assessment of disclosure risk in this case it then the $k$-anonymity rule. Most often it is assumed that $k=3$, although sometimes also $k=2$ or $k=5$ is applied. So, in the frequency tables the primary risk results only for direct identification of an unit resulting from too low frequency and the secondary risk assessment is connected with possibility of deduction of suppressed/perturbed cells by relevant differenciation of table.

Lupp and Langsrud (2021) describe the main idea of secondary risk in frequency tables as follows. Assume that an attacker has knowledge of $k$ (where $k$ is the natural number) contributors to the table (called *a $k$-coalition*). In the most basic case, an attacker has no background knowledge whatsoever, i.e., for him/her is $k=0$. This situation is, however, rather impossible in practice. The statistical unit know usually themself, and hence can identify cell which it belong to. Thus, most often, we have $k=1$. An attacker can then use their background knowledge to disclose information about other units by removing themselves from the data and analyzing the resulting table. Lupp and Langsrud (2021) regard a cell $c$ in a frequency table as *directly disclosive with reference to k*, if there exists a published marginal cell $pC$ within a sensitive variable, such that
• $C$ is a cell contributing to $pC$, and
• $|pC|-k\le |C|$,
where $|T|$ denotes the number of units belonging to the cell $T$. In other words, if a cell is directly disclosive with reference to $k$ then there exists an attacker with knowledge of k table contributors that can deduce that all other units contributing to pc must be in the cell $C$. Therefore, the share of directly disclosive cells in their total number can be a good measure of primary dislosure risk.  

Similarly as in the case of magnitude tables (see Section 4.6) a cell is regarded as unsafe if it violates the $k$-anonymity. Therefore the dislosure risk can be measured as the share of cells having this property in the total number of cells in a table.

As regards the secondary risk, its measure should satisfy four conditions formulated by Antal, Shlomo and Elliot (2014) and recalled in Section 4.6. Then the measure (4.6.1) and (4.6.2) can be applied directly in this case. They have proposed also a measure of risk for population tables. Let $F_l$ be population frequency of cell $C_l$, $l=1,2,\ldots,k$ (where $k$ is the total number of cells). Let $N$ be the number of population, $N=\sum_{l=1}^k{F_k}$ and $D=\{l\n\{1,2,\ldots,n\}:F_l=0\}$. The disclosure risk for a population based frequency table is then defined as
$$
r=w_1\freq{|D|}{k}+w_2\left(1-\frac{H}{\log k}\right)-w_3\frac{1}{\sqrt{N}}\log\frac{1}{e\cdot\sqrt{N}},
$$
where $|A|$ denotes the number of elements of the set $A$, $H$ is the value defined in (4.6.1) but adjusted to the current situation as 
$$
H=-\sum_{i=1}^{k}{\frac{F_i}{N}\cdot\log{\frac{F_i}{N}}},
$$
and $\mathbf{w}=(w_1,w_2,w_3)$ is the vector of weights ($e$ is, of course, the base of the natural logarithm). This measure has all above mentioned properties.

Assume that the original table was perturbed. The frequency of cell $C_l$ is then $F_l^#$, $l=1,2,\ldots,k$. Because the modified table has more uncertainty, the proposed measure of disclosure risk in this case is as follows: 
$$
r^#=w_1\left(\freq{|D|}{k}\right)^{\frac{|D\cup Q|}{|D\cap Q|}}+w_2\left(1-\frac{H}{\log k}\right)\left(1-\frac{H^#}{H}\right)-w_3\frac{1}{\sqrt{N}}\log\frac{1}{e\cdot\sqrt{N}}
$$
where $Q$ is the set of zeroes in the perturbed frequency table and $H^#=-\sum_{i=1}^{k}{\frac{F_i^#}{N}\cdot\sum_{j=1}^{k}{\frac{F_{ij}^#}{N\cdot F_j^#}\cdot\log{\frac{F_{ij}^#}{n\cdot F_j^#}}}$, $F_{ij}^#$ is the number of units which belong to $C_i$ before and to $C_j$ after perturbation. The disclosure risk of a perturbed table is expected to be lower than that of the original table. $r^#$ satisfies this requirement.

### Information loss
The process of reducing disclosure risk results also in information loss. Some quantitative information loss measures have been developed by Shlomo and Young (2005 & 2006) to determining the impact various statistical disclosure control (SDC) methods have on the original tables.

Information loss measures can be split into two classes: measures for data suppliers used to make informed decisions about optimal SDC methods depending on the characteristics of the tables and measures for users in order to facilitate adjustments to be made when carrying out statistical analysis on protected tables. The measures for data suppliers are usually, in fact, a collection of measures for users computed for various variants of SDC methods to assess which of them will be most useful. Measuring utility and quality for SDC methods is subjective. It depends on the users, the purpose of the data and the required statistical analysis and the type and format of the statistical data. Therefore it is useful to have a range of information loss measures for assessing the impact of the SDC methods.

The focus here is information loss measures for tables containing frequency counts, however, some of these measures can easily be adapted to microdata. Magnitude or weighted sample tables will have the additional element of the number of contributors to each cell of the table.

When evaluating information loss measures for tables protected using cell suppression, one needs to decide on an imputation method for replacing the suppressed cells similar to what one would expect a user to do prior to analysing the data (i.e. we need to measure the difference between the observed and actual values, and for suppressed cells the observed values will be based on user inference about the possible cell values). A naive user might use zeros in place of the suppressed cells whereas a more sophisticated user might replace suppressed cells by some form of averaging of the total information that was suppressed, or by calculating feasibility intervals. So, in this case the information loss is measure ind fact in relation to a kind of perturbation.

A number of different information loss measures are described below, and more technical details can be found e.g. in Shlomo and Young (2005 & 2006).

• An exact Binomial Hypothesis Test can be used to check if the realization of a random stochastic perturbation scheme, such as random rounding, follows the expected probabilities (i.e. the parameters of the method). For other SDC methods, a non-parametric signed rank test can be used to check whether the location of the empirical distribution after the application of the SDC method, has changed.
• Information loss measures that measure distortion to distributions are based on distance metrics between the original and perturbed cells. Some useful metrics are also presented in Gomatam and Karr (2003). A distance metric can be calculated for internal cells of a table. When combining several tables one may want to calculate an overall average across the tables as the information loss measure. These distance metrics can also be calculated for totals or sub-totals of the tables. One should use in this context the ideas and formulas presented in Section 4.6. The measures based on distances AD, RAD nad HD can be easy applied also to the case of frequency tables.   
• SDC methods will have an impact on the variance of the average cell size for the rows, columns or the entire table. The variance of the average cell size is examined before and after the SDC method has been applied. Another important variance to examine is the "between" variance when carrying out a one-way ANOVA test based on the table. In ANOVA, we examine the means of a specific target variable within groupings defined by independent categorical variables. The goodness of fit statistic $R^2$ for testing the null hypothesis that the means are equal across the groupings is based on the variance of the means between the groupings divided by the total variance. The information loss measure therefore examines the impact of the "between" variance and whether the means of the groupings have become more homogenized or spread apart as a result of the SDC method.
• Another statistical analysis tool that is frequently carried out on tabular data are tests for independence between categorical variables that span the table. The test for independence for a two-way table is based on a Pearson Chi-Squared Statistic and the measure of association is the Cramer's V statistic. For multi-way tables, one can examine conditional dependencies and calculate expected cell frequencies based on the theory of log-linear models. The test statistic for the fit of the model is also based on a Pearson Chi-Squared Statistic. SDC methods applied to tables may change the results of statistical inferences. Therefore we examine the impact to the test statistics before and after the application of the SDC method.
• A good statistical tool for inference is the Spearman's Rank Correlation. This is a technique that tests the direction and strength of the relationship between two variables. The statistic is based on ranking both sets of data from the highest to the lowest. Therefore, one important assessment of the impact of the SDC method on statistical data is whether we are distorting the rankings of the cell counts.
• One can use here in this context also the Kendall's Rank Correlation (also called the $\tau$-Kendall's Correlation). It is based on the number of concordant and discordant pairs of observations. If the order of elements in both pairs is the same then pairs are concordant, otherwise they are discordant. Therefore more detailed information on rank distribution than in the Spearman's coefficient is here taken into account. However, in both cases the actual differences between cell counts are largely overlooked.    

In order to allow data suppliers to make informed decisions about optimal disclosure control methods, ONS have developed a user-friendly software application that calculates both disclosure risk measures based on small counts in tables and a wide range of information loss measures (as described above) for disclosure controlled statistical data, Shlomo and Young (2006). The software application also outputs R-U Confidentiality maps.

### References
Antal, L., Shlomo, N., & Elliot, M. (2014), Measuring disclosure risk with entropy in population based frequency tables. In *Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings* (pp. 62-78). Springer International Publishing.

Antal, L., & Shlomo, N., & Elliot, M. (2015), Disclosure Risk Measurement with Entropy in Sample Based Frequency Tables, *New Techniques and Technologies for Statistics (NTTS) Reliable Evidence for a Society in Transition, Brussels, Belgium, 9-13 March 2015*, https://cros-legacy.ec.europa.eu/system/files/Antal-etal_NTTS%202015%20abstract%20unblinded%20disclosure%20risk%20measurement.pdf.

Gomatam, S. & A. Karr (2003), *Distortion Measures for Categorical Data Swapping*, Technical Report Number 131, National Institute of Statistical Sciences

Lupp, D. P., & Langsrud, Ø. (2021). Suppression of directly-disclosive cells in frequency tables. In *Joint UNECE/Eurostat Expert Meeting on Statistical Data Confidentiality, Poznań, 1-3 December 2021* (pp. 1-3).

Shlomo, N., & Young, C. (2005). Information loss measures for frequency tables. *Monographs of official statistics*, Eurostat, Luxembourg, pp. 277-289.

Shlomo, N., & Young, C. (2006). Statistical disclosure control methods through a risk-utility framework. In *Privacy in Statistical Databases: CENEX-SDC Project International Conference, PSD 2006, Rome, Italy, December 13-15, 2006. Proceedings* (pp. 68-81). Springer Berlin Heidelberg.
